{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90a05fb-9060-4387-8d34-8ca7428d8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================== Refactored & Extended anomaly analysis pipeline =========================================== #\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, confusion_matrix, classification_report,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Like the name says, an heartbeat, to check if the system is alive\n",
    "import threading, time\n",
    "\n",
    "def heartbeat():\n",
    "    while True:\n",
    "        time.sleep(3600)  # 1 hour\n",
    "        print(\"Running...\")\n",
    "\n",
    "thread = threading.Thread(target=heartbeat, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e2026-07fb-4fef-ab9e-62d0e4944681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0. Config / Paths\n",
    "# -------------------------\n",
    "TRAIN_PATH = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Testing_file.csv'\n",
    "TEST_PATH  = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Training_file.csv'\n",
    "LABEL_COL = \"Label\"   # set to your label column name if exists, otherwise leave as-is\n",
    "SAMPLE_FOR_SIL = 40000\n",
    "SIL_BATCH = 40000\n",
    "LOF_BATCH = 200_000\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0310b6c5-bccb-414d-9a24-78cac646b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Load data\n",
    "# -------------------------\n",
    "def load_datasets(train_path, test_path):\n",
    "    df_tr = pd.read_csv(train_path, encoding='ISO-8859-1')\n",
    "    df_te = pd.read_csv(test_path, encoding='ISO-8859-1')\n",
    "    return df_tr, df_te\n",
    "\n",
    "df_train, df_test = load_datasets(TRAIN_PATH, TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d510ad-aaa3-4ac7-9609-07afa1d90231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_15036\\1138178066.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train['Info'].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_15036\\1138178066.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['Info'].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Basic cleaning\n",
    "# -------------------------\n",
    "# Fill Info NaNs and drop rows missing Source/Destination\n",
    "df_train['Info'].fillna(\"Unknown\", inplace=True)\n",
    "df_test['Info'].fillna(\"Unknown\", inplace=True)\n",
    "df_train.dropna(subset=['Source','Destination'], inplace=True)\n",
    "df_test.dropna(subset=['Source','Destination'], inplace=True)\n",
    "\n",
    "# Remove rows with negative Time (if that's desired)\n",
    "df_train = df_train[df_train['Time'] >= 0].copy()\n",
    "df_test  = df_test[df_test['Time'] >= 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b69cd9b-d061-4bb9-aae6-290248ba91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Build global mappings ==================== #\n",
    "def build_mapping(train_col, test_col, name):\n",
    "    \"\"\"Create a dictionary mapping for train+test, save to JSON, return mapping.\"\"\"\n",
    "    all_values = pd.concat([train_col, test_col]).unique()\n",
    "    mapping = {val: idx for idx, val in enumerate(all_values)}\n",
    "\n",
    "    # optional: save for reproducibility\n",
    "    with open(f\"{name}_mapping.json\", \"w\") as f:\n",
    "        json.dump(mapping, f)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# Protocol\n",
    "protocol_mapping = build_mapping(df_train['Protocol'], df_test['Protocol'], \"protocol\")\n",
    "df_train['Protocol_enc'] = df_train['Protocol'].map(protocol_mapping).fillna(-1).astype(int)\n",
    "df_test['Protocol_enc']  = df_test['Protocol'].map(protocol_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Source\n",
    "source_mapping = build_mapping(df_train['Source'], df_test['Source'], \"source\")\n",
    "df_train['Source_enc'] = df_train['Source'].map(source_mapping).fillna(-1).astype(int)\n",
    "df_test['Source_enc']  = df_test['Source'].map(source_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Destination\n",
    "dest_mapping = build_mapping(df_train['Destination'], df_test['Destination'], \"destination\")\n",
    "df_train['Destination_enc'] = df_train['Destination'].map(dest_mapping).fillna(-1).astype(int)\n",
    "df_test['Destination_enc']  = df_test['Destination'].map(dest_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# ==================== Drop originals (optional) ==================== #\n",
    "# Keep only the encoded versions for modeling\n",
    "df_train.drop(['Protocol', 'Source', 'Destination'], axis=1, inplace=True)\n",
    "df_test.drop(['Protocol', 'Source', 'Destination'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50451316-44d3-4946-bf48-bc17b6199ad0",
   "metadata": {},
   "source": [
    "# Protocol\n",
    "df_train['Protocol_enc'] = df_train['Protocol'].astype('category').cat.codes\n",
    "df_test['Protocol_enc']  = df_test['Protocol'].astype('category').cat.codes\n",
    "\n",
    "# Source\n",
    "df_train['Source_enc'] = df_train['Source'].astype('category').cat.codes\n",
    "df_test['Source_enc']  = df_test['Source'].astype('category').cat.codes\n",
    "\n",
    "# Destination\n",
    "df_train['Destination_enc'] = df_train['Destination'].astype('category').cat.codes\n",
    "df_test['Destination_enc']  = df_test['Destination'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3148048b-bdd8-4ef9-9570-bfb49134ec2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoxCox_PRate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPacket_Rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 39\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m df_test  \u001b[38;5;241m=\u001b[39m build_features(df_test)\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mbuild_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_features\u001b[39m(df):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Time diff per Source\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_Diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# If first diff is NaN, fill with group's median diff (safer than mean)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_Diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_Diff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Source'"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 4. Feature engineering (as in your code, with small safety tweaks)\n",
    "# -------------------------\n",
    "def build_features(df):\n",
    "    # Time diff per Source\n",
    "    df['Time_Diff'] = df.groupby(\"Source\")['Time'].diff()\n",
    "    # If first diff is NaN, fill with group's median diff (safer than mean)\n",
    "    df['Time_Diff'] = df['Time_Diff'].fillna(df.groupby('Source')['Time'].transform('median'))\n",
    "    df['Time_Diff'] = df['Time_Diff'].fillna(0.0)\n",
    "    # Packet rate: robust clip then packets / time span (clip prevents tiny denominators)\n",
    "    def packet_rate(series):\n",
    "        if len(series) < 2:\n",
    "            return 0.0\n",
    "        q1, q3 = np.percentile(series, [25, 75])\n",
    "        iqr = max(q3 - q1, 1e-9)\n",
    "        clipped = np.clip(series, q1 - 1.5 * iqr, q3 + 1.5 * iqr)\n",
    "        denom = clipped.max() - clipped.min()\n",
    "        denom = denom if denom > 1e-6 else 1e-6\n",
    "        return len(clipped) / denom\n",
    "    df['Packet_Rate'] = df.groupby('Source')['Time'].transform(packet_rate)\n",
    "    # Inter-arrival (rolling mean)\n",
    "    df['Inter_Arrival_Time'] = df.groupby('Source')['Time_Diff'].transform(lambda x: x.rolling(10, min_periods=1).mean())\n",
    "    df['Inter_Arrival_Time'] = df['Inter_Arrival_Time'].clip(lower=1e-6)\n",
    "    df['Burst_Rate'] = np.where(df['Inter_Arrival_Time'] > 1e-6, 1.0 / df['Inter_Arrival_Time'], 0.0)\n",
    "    # Transforms\n",
    "    df['Log_IATime'] = np.log1p(df['Inter_Arrival_Time'])\n",
    "    df['Log_BRate']  = np.log1p(df['Burst_Rate'].clip(lower=0))\n",
    "    # BoxCox requires positive. add small epsilon\n",
    "    df['BoxCox_Length'] = df['Length'] + 1e-3\n",
    "    df['BoxCox_Length'], _ = boxcox(df['BoxCox_Length'])\n",
    "    df['BoxCox_PRate'] = df['Packet_Rate'] + 1e-6\n",
    "    # BoxCox may fail if constant; guard with log1p fallback\n",
    "    try:\n",
    "        df['BoxCox_PRate'], _ = boxcox(df['BoxCox_PRate'])\n",
    "    except Exception:\n",
    "        df['BoxCox_PRate'] = np.log1p(df['Packet_Rate'])\n",
    "    return df\n",
    "\n",
    "df_train = build_features(df_train)\n",
    "df_test  = build_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea00887-950c-4e7b-9a41-dc30445523f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5. Select features & scale (fit scaler on train, apply to test)\n",
    "# -------------------------\n",
    "NUM_FEATURES = [\"Time_Diff\", \"Log_IATime\", \"Log_BRate\", \"BoxCox_Length\", \"BoxCox_PRate\"]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train[NUM_FEATURES])\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train[NUM_FEATURES]), columns=NUM_FEATURES)\n",
    "df_test_scaled  = pd.DataFrame(scaler.transform(df_test[NUM_FEATURES]), columns=NUM_FEATURES)\n",
    "\n",
    "# Small de-duplication + tiny noise to avoid LOF duplicate issues\n",
    "df_train_scaled = df_train_scaled.drop_duplicates().reset_index(drop=True)\n",
    "df_test_scaled  = df_test_scaled.drop_duplicates().reset_index(drop=True)\n",
    "df_train_scaled += np.random.normal(0, 1e-6, df_train_scaled.shape).astype(\"float32\")\n",
    "df_test_scaled  += np.random.normal(0, 1e-6, df_test_scaled.shape).astype(\"float32\")\n",
    "\n",
    "# Reduce precision to save memory\n",
    "df_train_scaled = df_train_scaled.astype(\"float32\")\n",
    "df_test_scaled  = df_test_scaled.astype(\"float32\")\n",
    "\n",
    "# We'll keep copies of the scaled frames with indexes aligned to original for backreference:\n",
    "df_train_scaled_full = df_train_scaled.copy()\n",
    "df_test_scaled_full  = df_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac351d97-c371-4635-b0a5-00a911196352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6. Train Isolation Forest\n",
    "# -------------------------\n",
    "iso_forest = IsolationForest(n_estimators=300, contamination=\"auto\", max_samples=100000, random_state=RANDOM_STATE, verbose=0)\n",
    "t0 = time.time()\n",
    "iso_forest.fit(df_train_scaled_full)\n",
    "t1 = time.time()\n",
    "print(f\"IsolationForest trained in {t1-t0:.1f}s\")\n",
    "\n",
    "df_train_scaled_full[\"Anomaly_IForest\"] = iso_forest.predict(df_train_scaled_full)\n",
    "df_test_scaled_full  = df_test_scaled_full if 'df_test_scaled_full' in globals() else df_test_scaled.copy()\n",
    "df_test_scaled_full[\"Anomaly_IForest\"] = iso_forest.predict(df_test_scaled)\n",
    "\n",
    "# Convert labels to -1/1 convention (already so)\n",
    "print(\"IForest train anomaly %:\", (df_train_scaled_full[\"Anomaly_IForest\"]==-1).mean()*100)\n",
    "print(\"IForest test anomaly %: \", (df_test_scaled_full[\"Anomaly_IForest\"]==-1).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca6aa3-f7e1-4888-8cfa-c2f1e0ce8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Batched LOF predictions (memory-friendly)\n",
    "# -------------------------\n",
    "lof = LocalOutlierFactor(n_neighbors=30, algorithm=\"kd_tree\", leaf_size=40, contamination=\"auto\", metric=\"euclidean\", novelty=False, n_jobs=-1)\n",
    "\n",
    "def lof_predict_batched(X, batch_size=LOF_BATCH, lof_model=None):\n",
    "    \"\"\"Return concatenated lof predictions computed per-batch.\n",
    "       NOTE: LOF.fit_predict computes scores per-batch independently (no incremental LOF).\"\"\"\n",
    "    if lof_model is None:\n",
    "        lof_model = LocalOutlierFactor(n_neighbors=30, metric=\"euclidean\", contamination=\"auto\")\n",
    "    n = len(X)\n",
    "    y = np.zeros(n, dtype=int)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = X.iloc[i:i+batch_size]\n",
    "        # fit_predict returns -1 for outlier, 1 for inlier\n",
    "        y[i:i+len(batch)] = lof_model.fit_predict(batch)\n",
    "    return y\n",
    "\n",
    "t0 = time.time()\n",
    "y_lof_train = lof_predict_batched(df_train_scaled_full, batch_size=LOF_BATCH, lof_model=lof)\n",
    "t1 = time.time()\n",
    "print(f\"LOF (train) batched predictions in {t1-t0:.1f}s\")\n",
    "\n",
    "y_lof_test = lof_predict_batched(df_test_scaled, batch_size=LOF_BATCH, lof_model=lof)\n",
    "df_train_scaled_full[\"Anomaly_LOF\"] = y_lof_train\n",
    "df_test_scaled_full[\"Anomaly_LOF\"] = y_lof_test\n",
    "\n",
    "print(\"LOF train anomaly %:\", (df_train_scaled_full[\"Anomaly_LOF\"]==-1).mean()*100)\n",
    "print(\"LOF test anomaly %: \", (df_test_scaled_full[\"Anomaly_LOF\"]==-1).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf14583-949a-4cc1-ae97-edfa4d1d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8. Agreement & simple stats\n",
    "# -------------------------\n",
    "agreement_train = (df_train_scaled_full[\"Anomaly_IForest\"] == df_train_scaled_full[\"Anomaly_LOF\"]).mean()\n",
    "print(f\"Model agreement on train: {agreement_train:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597b373-8908-49b1-8247-e8227988714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 9. Silhouette (batched sampling)\n",
    "# -------------------------\n",
    "def batched_silhouette(X, labels_col, batch_size=SAMPLE_FOR_SIL, metric='euclidean', n_batches=5):\n",
    "    scores = []\n",
    "    n = len(X)\n",
    "    # sample n_batches different random batches\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    for i in range(n_batches):\n",
    "        sample_idx = rng.choice(n, min(batch_size, n), replace=False)\n",
    "        batch = X.iloc[sample_idx]\n",
    "        labels = batch[labels_col].values\n",
    "        # silhouette requires at least 2 clusters with more than 1 sample each\n",
    "        if len(np.unique(labels)) < 2 or min(np.bincount((labels==-1).astype(int))) < 2:\n",
    "            scores.append(np.nan)\n",
    "            continue\n",
    "        try:\n",
    "            sc = silhouette_score(batch[NUM_FEATURES], labels, metric=metric)\n",
    "            scores.append(sc)\n",
    "        except Exception:\n",
    "            scores.append(np.nan)\n",
    "    return np.array(scores)\n",
    "\n",
    "sil_if = batched_silhouette(df_train_scaled_full.assign(**{ 'labels_if' : df_train_scaled_full[\"Anomaly_IForest\"]}), 'Anomaly_IForest', batch_size=SAMPLE_FOR_SIL, n_batches=5, metric='euclidean')\n",
    "sil_lof= batched_silhouette(df_train_scaled_full.assign(**{ 'labels_lof': df_train_scaled_full[\"Anomaly_LOF\"]}), 'Anomaly_LOF', batch_size=SAMPLE_FOR_SIL, n_batches=5, metric='euclidean')\n",
    "\n",
    "print(\"Silhouette IForest (mean of batches):\", np.nanmean(sil_if))\n",
    "print(\"Silhouette LOF     (mean of batches):\", np.nanmean(sil_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabd6d8-e465-4e82-a8d8-8fdc0fca5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 10. Feature-level summaries & plots (KDEs etc)\n",
    "# -------------------------\n",
    "def feature_summary_and_plots(df_scaled_full, original_df, model_cols, features):\n",
    "    \"\"\"For each model in model_cols, produce feature summaries and KDEs (side-by-side).\"\"\"\n",
    "    summaries = {}\n",
    "    for model_col in model_cols:\n",
    "        summary = {}\n",
    "        summary['anomaly_rate'] = (df_scaled_full[model_col] == -1).mean() * 100\n",
    "        feat_stats = {}\n",
    "        for f in features:\n",
    "            # map back scaled -> original feature values roughly using the scaler inverse\n",
    "            # but we also can read original_df to get real values aligned: careful w/ dedup.\n",
    "            # For simplicity we compute stats from original_df filtered by mask aligned via index if same size.\n",
    "            mask = (df_scaled_full[model_col] == -1)\n",
    "            feat_stats[f] = {\n",
    "                'mean_normal': original_df.loc[~mask, f].mean() if f in original_df.columns else np.nan,\n",
    "                'mean_anomaly': original_df.loc[mask, f].mean() if f in original_df.columns else np.nan,\n",
    "                'median_normal': original_df.loc[~mask, f].median() if f in original_df.columns else np.nan,\n",
    "                'median_anomaly': original_df.loc[mask, f].median() if f in original_df.columns else np.nan,\n",
    "                'anomaly_count': mask.sum()\n",
    "            }\n",
    "        summary['feature_stats'] = feat_stats\n",
    "        summaries[model_col] = summary\n",
    "\n",
    "    # Plots: KDE side-by-side for each feature\n",
    "    for f in features:\n",
    "        fig, axes = plt.subplots(1, len(model_cols), figsize=(5 * len(model_cols), 4))\n",
    "        if len(model_cols) == 1:\n",
    "            axes = [axes]\n",
    "        for ax, model_col in zip(axes, model_cols):\n",
    "            mask_an = df_scaled_full[model_col] == -1\n",
    "            mask_no = df_scaled_full[model_col] == 1\n",
    "            # attempt to plot using original_df if possible to keep units\n",
    "            data_normal = original_df.loc[mask_no, f] if f in original_df.columns else df_scaled_full.loc[mask_no, f]\n",
    "            data_anom   = original_df.loc[mask_an, f] if f in original_df.columns else df_scaled_full.loc[mask_an, f]\n",
    "            if data_normal.dropna().shape[0] > 1:\n",
    "                sns.kdeplot(data_normal, ax=ax, label='Normal', fill=True, color='blue')\n",
    "            if data_anom.dropna().shape[0] > 1:\n",
    "                sns.kdeplot(data_anom, ax=ax, label='Anomaly', fill=True, color='red')\n",
    "            ax.set_title(f\"{model_col} - {f}\")\n",
    "            ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return summaries\n",
    "\n",
    "# Build summaries for train (use original df_train for real feature units)\n",
    "model_cols = [\"Anomaly_IForest\", \"Anomaly_LOF\"]\n",
    "train_summaries = feature_summary_and_plots(df_train_scaled_full, df_train.reset_index(drop=True), model_cols, [\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "print(\"Train summaries done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f972f2-dd12-4b0c-b930-6c6baeb8c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 11. If labels exist -> compute confusion & per-feature TP/FP/FN/TN\n",
    "# -------------------------\n",
    "def compute_confusion_and_feature_errors(original_df, df_scaled_full, model_col, label_col=LABEL_COL, features=NUM_FEATURES):\n",
    "    \"\"\"If ground truth exists, compute confusion and per-feature breakdown.\"\"\"\n",
    "    if label_col not in original_df.columns:\n",
    "        return None\n",
    "    # Normalize label: try to map -1/1 or 0/1 to 1=anomaly\n",
    "    y_true = original_df[label_col].copy().astype(int)\n",
    "    # If labels are 1 (normal) 0 (anomaly) detect and transform: assume anomalies are 1 when values {-1,1} present\n",
    "    if set(y_true.unique()) <= {0,1}:  # 0/1, assume 1=anomaly? ambiguous -> attempt to detect majority\n",
    "        # we assume 1 means anomaly only if anomalies are minority; otherwise try -1/1\n",
    "        pass\n",
    "    # We'll make y_true_binary such that 1=anomaly, 0=normal\n",
    "    if set(y_true.unique()) == {-1,1}:\n",
    "        y_true_bin = (y_true == -1).astype(int)\n",
    "    elif set(y_true.unique()) <= {0,1}:\n",
    "        # assume 1 == anomaly if anomaly labels are rare; otherwise ask user — but here we map 1 -> anomaly\n",
    "        y_true_bin = y_true.copy()\n",
    "    else:\n",
    "        # unknown encoding\n",
    "        y_true_bin = y_true.copy()\n",
    "\n",
    "    y_pred = (df_scaled_full[model_col] == -1).astype(int)\n",
    "    cm = confusion_matrix(y_true_bin[:len(y_pred)], y_pred)\n",
    "    report = classification_report(y_true_bin[:len(y_pred)], y_pred, digits=4)\n",
    "    # per-feature error rates: compute FP rate among samples where feature in top quantile, etc.\n",
    "    per_feature = {}\n",
    "    for feat in features:\n",
    "        if feat in original_df.columns:\n",
    "            q75 = original_df[feat].quantile(0.75)\n",
    "            high_mask = original_df[feat] >= q75\n",
    "            # in this high-value subgroup: compute precision/recall\n",
    "            if high_mask.sum() > 0:\n",
    "                y_t = y_true_bin[high_mask]\n",
    "                y_p = y_pred[high_mask][:len(y_t)]\n",
    "                prec, rec, f1, _ = precision_recall_fscore_support(y_t, y_p, average='binary', zero_division=0)\n",
    "            else:\n",
    "                prec, rec, f1 = np.nan, np.nan, np.nan\n",
    "            per_feature[feat] = {'q75_count': int(high_mask.sum()), 'precision_high': prec, 'recall_high': rec, 'f1_high': f1}\n",
    "    return {'confusion_matrix': cm, 'classification_report': report, 'per_feature_high': per_feature}\n",
    "\n",
    "# check for train labels\n",
    "train_conf = compute_confusion_and_feature_errors(df_train.reset_index(drop=True), df_train_scaled_full, 'Anomaly_IForest', LABEL_COL, features=[\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "if train_conf:\n",
    "    print(\"Confusion and feature-level errors (train):\")\n",
    "    print(train_conf['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8f251-e74e-40bd-a768-3da9b5c478d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 12. Permutation importance for feature relevance (IsolationForest)\n",
    "# -------------------------\n",
    "# Permutation importance needs a predict_proba-like continuous score. For IsolationForest we can use decision_function\n",
    "try:\n",
    "    iso_scores = iso_forest.decision_function(df_train_scaled_full[NUM_FEATURES])\n",
    "    r = permutation_importance(iso_forest, df_train_scaled_full[NUM_FEATURES], iso_scores, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    imp_df = pd.DataFrame({'feature': NUM_FEATURES, 'importance_mean': r.importances_mean, 'importance_std': r.importances_std})\n",
    "    imp_df.sort_values('importance_mean', ascending=False, inplace=True)\n",
    "    print(\"Permutation importances (IForest):\")\n",
    "    print(imp_df)\n",
    "except Exception as e:\n",
    "    print(\"Permutation importance failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207ac2b-152f-4354-aea5-8d106d888553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 13. Dotted summary plot: per-feature comparison (anomaly rates and (if available) proxy precision)\n",
    "# -------------------------\n",
    "def dotted_summary_plot(df_scaled_full, original_df, model_cols, features):\n",
    "    recs = []\n",
    "    for model_col in model_cols:\n",
    "        for f in features:\n",
    "            mask_an = df_scaled_full[model_col] == -1\n",
    "            # anomaly rate in top quantile for this feature\n",
    "            if f in original_df.columns:\n",
    "                q75 = original_df[f].quantile(0.75)\n",
    "                top = original_df[f] >= q75\n",
    "                # proportion of anomalies within top quantile:\n",
    "                prop_in_top = (mask_an[top.values]).sum() / max(1, top.sum())\n",
    "            else:\n",
    "                prop_in_top = np.nan\n",
    "            recs.append({'model': model_col, 'feature': f, 'anomaly_rate': (mask_an.mean()*100),'prop_top_q75': prop_in_top})\n",
    "    df_plot = pd.DataFrame(recs)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for model, g in df_plot.groupby('model'):\n",
    "        plt.scatter(g['feature'], g['anomaly_rate'], s=100, label=model)\n",
    "    plt.ylabel('Anomaly Rate (%)')\n",
    "    plt.title('Per-feature anomaly rates (by model)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return df_plot\n",
    "\n",
    "df_plot = dotted_summary_plot(df_train_scaled_full, df_train.reset_index(drop=True), model_cols, [\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "print(\"Dotted summary table:\")\n",
    "print(df_plot)\n",
    "\n",
    "# -------------------------\n",
    "# End - prints and small guidance\n",
    "# -------------------------\n",
    "print(\"Done. Key artifacts:\")\n",
    "print(f\" - df_train_scaled_full, df_test_scaled_full contain scaled features and model labels.\")\n",
    "print(\" - train_summaries contains per-model feature stats returned earlier.\")\n",
    "print(\" - df_plot contains the dotted-summary aggregated table.\")\n",
    "print(\"Next: inspect train_summaries and df_plot, examine KDEs and permutation importances.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
