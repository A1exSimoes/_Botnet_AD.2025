{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90a05fb-9060-4387-8d34-8ca7428d8485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n",
      "⏳ Still running...\n"
     ]
    }
   ],
   "source": [
    "# Refactored & Extended anomaly analysis pipeline\n",
    "# Paste into a Jupyter cell (ensure required libs are installed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, confusion_matrix, classification_report,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Like the name says, an heartbeat, to check if the system is alive\n",
    "import threading, time\n",
    "\n",
    "def heartbeat():\n",
    "    while True:\n",
    "        time.sleep(120)  # 2 minutes\n",
    "        print(\"⏳  Still running...\")\n",
    "\n",
    "thread = threading.Thread(target=heartbeat, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e2026-07fb-4fef-ab9e-62d0e4944681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0. Config / Paths\n",
    "# -------------------------\n",
    "TRAIN_PATH = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Testing_file.csv'\n",
    "TEST_PATH  = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Training_file.csv'\n",
    "LABEL_COL = \"Label\"   # set to your label column name if exists, otherwise leave as-is\n",
    "SAMPLE_FOR_SIL = 40000\n",
    "SIL_BATCH = 40000\n",
    "LOF_BATCH = 200_000\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0310b6c5-bccb-414d-9a24-78cac646b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Load data\n",
    "# -------------------------\n",
    "def load_datasets(train_path, test_path):\n",
    "    df_tr = pd.read_csv(train_path, encoding='ISO-8859-1')\n",
    "    df_te = pd.read_csv(test_path, encoding='ISO-8859-1')\n",
    "    return df_tr, df_te\n",
    "\n",
    "df_train, df_test = load_datasets(TRAIN_PATH, TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d510ad-aaa3-4ac7-9609-07afa1d90231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_16544\\1138178066.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train['Info'].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\alexs\\AppData\\Local\\Temp\\ipykernel_16544\\1138178066.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['Info'].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. Basic cleaning\n",
    "# -------------------------\n",
    "# Fill Info NaNs and drop rows missing Source/Destination\n",
    "df_train['Info'].fillna(\"Unknown\", inplace=True)\n",
    "df_test['Info'].fillna(\"Unknown\", inplace=True)\n",
    "df_train.dropna(subset=['Source','Destination'], inplace=True)\n",
    "df_test.dropna(subset=['Source','Destination'], inplace=True)\n",
    "\n",
    "# Remove rows with negative Time (if that's desired)\n",
    "df_train = df_train[df_train['Time'] >= 0].copy()\n",
    "df_test  = df_test[df_test['Time'] >= 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f25582-13e2-4f01-8e80-95e58de3c56e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_t, np\u001b[38;5;241m.\u001b[39marray(mapped), le\n\u001b[0;32m     17\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol_enc\u001b[39m\u001b[38;5;124m'\u001b[39m], df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol_enc\u001b[39m\u001b[38;5;124m'\u001b[39m], le_protocol \u001b[38;5;241m=\u001b[39m safe_label_encode(df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m'\u001b[39m], df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 18\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource_enc\u001b[39m\u001b[38;5;124m'\u001b[39m],   df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource_enc\u001b[39m\u001b[38;5;124m'\u001b[39m],   le_source   \u001b[38;5;241m=\u001b[39m \u001b[43msafe_label_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination_enc\u001b[39m\u001b[38;5;124m'\u001b[39m], df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination_enc\u001b[39m\u001b[38;5;124m'\u001b[39m], le_dest \u001b[38;5;241m=\u001b[39m safe_label_encode(df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m'\u001b[39m], df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36msafe_label_encode\u001b[1;34m(train_ser, test_ser)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m test_ser\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[1;32m---> 12\u001b[0m         mapped\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         mapped\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(values\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:173\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(values, uniques)\n\u001b[1;32m--> 173\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m_nandict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:160\u001b[0m, in \u001b[0;36m_nandict.__init__\u001b[1;34m(self, mapping)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(mapping)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_scalar_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_missing.py:41\u001b[0m, in \u001b[0;36mis_scalar_nan\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scalar_nan\u001b[39m(x):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Test if x is NaN.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    This function is meant to overcome the issue that np.isnan does not allow\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 41\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, numbers\u001b[38;5;241m.\u001b[39mIntegral)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, numbers\u001b[38;5;241m.\u001b[39mReal)\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(x)\n\u001b[0;32m     44\u001b[0m     )\n",
      "File \u001b[1;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 3. Safe categorical encoding (fit on train -> transform test)\n",
    "# -------------------------\n",
    "def safe_label_encode(train_ser, test_ser):\n",
    "    le = LabelEncoder()\n",
    "    train_t = le.fit_transform(train_ser.astype(str))\n",
    "    # For test, map unseen to -1\n",
    "    classes = set(le.classes_)\n",
    "    mapped = []\n",
    "    for v in test_ser.astype(str):\n",
    "        if v in classes:\n",
    "            mapped.append(int(le.transform([v])[0]))\n",
    "        else:\n",
    "            mapped.append(-1)\n",
    "    return train_t, np.array(mapped), le\n",
    "\n",
    "df_train['Protocol_enc'], df_test['Protocol_enc'], le_protocol = safe_label_encode(df_train['Protocol'], df_test['Protocol'])\n",
    "df_train['Source_enc'],   df_test['Source_enc'],   le_source   = safe_label_encode(df_train['Source'], df_test['Source'])\n",
    "df_train['Destination_enc'], df_test['Destination_enc'], le_dest = safe_label_encode(df_train['Destination'], df_test['Destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148048b-bdd8-4ef9-9570-bfb49134ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. Feature engineering (as in your code, with small safety tweaks)\n",
    "# -------------------------\n",
    "def build_features(df):\n",
    "    # Time diff per Source\n",
    "    df['Time_Diff'] = df.groupby(\"Source\")['Time'].diff()\n",
    "    # If first diff is NaN, fill with group's median diff (safer than mean)\n",
    "    df['Time_Diff'] = df['Time_Diff'].fillna(df.groupby('Source')['Time'].transform('median'))\n",
    "    df['Time_Diff'] = df['Time_Diff'].fillna(0.0)\n",
    "    # Packet rate: robust clip then packets / time span (clip prevents tiny denominators)\n",
    "    def packet_rate(series):\n",
    "        if len(series) < 2:\n",
    "            return 0.0\n",
    "        q1, q3 = np.percentile(series, [25, 75])\n",
    "        iqr = max(q3 - q1, 1e-9)\n",
    "        clipped = np.clip(series, q1 - 1.5 * iqr, q3 + 1.5 * iqr)\n",
    "        denom = clipped.max() - clipped.min()\n",
    "        denom = denom if denom > 1e-6 else 1e-6\n",
    "        return len(clipped) / denom\n",
    "    df['Packet_Rate'] = df.groupby('Source')['Time'].transform(packet_rate)\n",
    "    # Inter-arrival (rolling mean)\n",
    "    df['Inter_Arrival_Time'] = df.groupby('Source')['Time_Diff'].transform(lambda x: x.rolling(10, min_periods=1).mean())\n",
    "    df['Inter_Arrival_Time'] = df['Inter_Arrival_Time'].clip(lower=1e-6)\n",
    "    df['Burst_Rate'] = np.where(df['Inter_Arrival_Time'] > 1e-6, 1.0 / df['Inter_Arrival_Time'], 0.0)\n",
    "    # Transforms\n",
    "    df['Log_IATime'] = np.log1p(df['Inter_Arrival_Time'])\n",
    "    df['Log_BRate']  = np.log1p(df['Burst_Rate'].clip(lower=0))\n",
    "    # BoxCox requires positive. add small epsilon\n",
    "    df['BoxCox_Length'] = df['Length'] + 1e-3\n",
    "    df['BoxCox_Length'], _ = boxcox(df['BoxCox_Length'])\n",
    "    df['BoxCox_PRate'] = df['Packet_Rate'] + 1e-6\n",
    "    # BoxCox may fail if constant; guard with log1p fallback\n",
    "    try:\n",
    "        df['BoxCox_PRate'], _ = boxcox(df['BoxCox_PRate'])\n",
    "    except Exception:\n",
    "        df['BoxCox_PRate'] = np.log1p(df['Packet_Rate'])\n",
    "    return df\n",
    "\n",
    "df_train = build_features(df_train)\n",
    "df_test  = build_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea00887-950c-4e7b-9a41-dc30445523f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5. Select features & scale (fit scaler on train, apply to test)\n",
    "# -------------------------\n",
    "NUM_FEATURES = [\"Time_Diff\", \"Log_IATime\", \"Log_BRate\", \"BoxCox_Length\", \"BoxCox_PRate\"]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train[NUM_FEATURES])\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train[NUM_FEATURES]), columns=NUM_FEATURES)\n",
    "df_test_scaled  = pd.DataFrame(scaler.transform(df_test[NUM_FEATURES]), columns=NUM_FEATURES)\n",
    "\n",
    "# Small de-duplication + tiny noise to avoid LOF duplicate issues\n",
    "df_train_scaled = df_train_scaled.drop_duplicates().reset_index(drop=True)\n",
    "df_test_scaled  = df_test_scaled.drop_duplicates().reset_index(drop=True)\n",
    "df_train_scaled += np.random.normal(0, 1e-6, df_train_scaled.shape).astype(\"float32\")\n",
    "df_test_scaled  += np.random.normal(0, 1e-6, df_test_scaled.shape).astype(\"float32\")\n",
    "\n",
    "# Reduce precision to save memory\n",
    "df_train_scaled = df_train_scaled.astype(\"float32\")\n",
    "df_test_scaled  = df_test_scaled.astype(\"float32\")\n",
    "\n",
    "# We'll keep copies of the scaled frames with indexes aligned to original for backreference:\n",
    "df_train_scaled_full = df_train_scaled.copy()\n",
    "df_test_scaled_full  = df_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac351d97-c371-4635-b0a5-00a911196352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6. Train Isolation Forest\n",
    "# -------------------------\n",
    "iso_forest = IsolationForest(n_estimators=300, contamination=\"auto\", max_samples=100000, random_state=RANDOM_STATE, verbose=0)\n",
    "t0 = time.time()\n",
    "iso_forest.fit(df_train_scaled_full)\n",
    "t1 = time.time()\n",
    "print(f\"IsolationForest trained in {t1-t0:.1f}s\")\n",
    "\n",
    "df_train_scaled_full[\"Anomaly_IForest\"] = iso_forest.predict(df_train_scaled_full)\n",
    "df_test_scaled_full  = df_test_scaled_full if 'df_test_scaled_full' in globals() else df_test_scaled.copy()\n",
    "df_test_scaled_full[\"Anomaly_IForest\"] = iso_forest.predict(df_test_scaled)\n",
    "\n",
    "# Convert labels to -1/1 convention (already so)\n",
    "print(\"IForest train anomaly %:\", (df_train_scaled_full[\"Anomaly_IForest\"]==-1).mean()*100)\n",
    "print(\"IForest test anomaly %: \", (df_test_scaled_full[\"Anomaly_IForest\"]==-1).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca6aa3-f7e1-4888-8cfa-c2f1e0ce8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Batched LOF predictions (memory-friendly)\n",
    "# -------------------------\n",
    "lof = LocalOutlierFactor(n_neighbors=30, algorithm=\"kd_tree\", leaf_size=40, contamination=\"auto\", metric=\"euclidean\", novelty=False, n_jobs=-1)\n",
    "\n",
    "def lof_predict_batched(X, batch_size=LOF_BATCH, lof_model=None):\n",
    "    \"\"\"Return concatenated lof predictions computed per-batch.\n",
    "       NOTE: LOF.fit_predict computes scores per-batch independently (no incremental LOF).\"\"\"\n",
    "    if lof_model is None:\n",
    "        lof_model = LocalOutlierFactor(n_neighbors=30, metric=\"euclidean\", contamination=\"auto\")\n",
    "    n = len(X)\n",
    "    y = np.zeros(n, dtype=int)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = X.iloc[i:i+batch_size]\n",
    "        # fit_predict returns -1 for outlier, 1 for inlier\n",
    "        y[i:i+len(batch)] = lof_model.fit_predict(batch)\n",
    "    return y\n",
    "\n",
    "t0 = time.time()\n",
    "y_lof_train = lof_predict_batched(df_train_scaled_full, batch_size=LOF_BATCH, lof_model=lof)\n",
    "t1 = time.time()\n",
    "print(f\"LOF (train) batched predictions in {t1-t0:.1f}s\")\n",
    "\n",
    "y_lof_test = lof_predict_batched(df_test_scaled, batch_size=LOF_BATCH, lof_model=lof)\n",
    "df_train_scaled_full[\"Anomaly_LOF\"] = y_lof_train\n",
    "df_test_scaled_full[\"Anomaly_LOF\"] = y_lof_test\n",
    "\n",
    "print(\"LOF train anomaly %:\", (df_train_scaled_full[\"Anomaly_LOF\"]==-1).mean()*100)\n",
    "print(\"LOF test anomaly %: \", (df_test_scaled_full[\"Anomaly_LOF\"]==-1).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf14583-949a-4cc1-ae97-edfa4d1d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8. Agreement & simple stats\n",
    "# -------------------------\n",
    "agreement_train = (df_train_scaled_full[\"Anomaly_IForest\"] == df_train_scaled_full[\"Anomaly_LOF\"]).mean()\n",
    "print(f\"Model agreement on train: {agreement_train:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597b373-8908-49b1-8247-e8227988714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 9. Silhouette (batched sampling)\n",
    "# -------------------------\n",
    "def batched_silhouette(X, labels_col, batch_size=SAMPLE_FOR_SIL, metric='euclidean', n_batches=5):\n",
    "    scores = []\n",
    "    n = len(X)\n",
    "    # sample n_batches different random batches\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    for i in range(n_batches):\n",
    "        sample_idx = rng.choice(n, min(batch_size, n), replace=False)\n",
    "        batch = X.iloc[sample_idx]\n",
    "        labels = batch[labels_col].values\n",
    "        # silhouette requires at least 2 clusters with more than 1 sample each\n",
    "        if len(np.unique(labels)) < 2 or min(np.bincount((labels==-1).astype(int))) < 2:\n",
    "            scores.append(np.nan)\n",
    "            continue\n",
    "        try:\n",
    "            sc = silhouette_score(batch[NUM_FEATURES], labels, metric=metric)\n",
    "            scores.append(sc)\n",
    "        except Exception:\n",
    "            scores.append(np.nan)\n",
    "    return np.array(scores)\n",
    "\n",
    "sil_if = batched_silhouette(df_train_scaled_full.assign(**{ 'labels_if' : df_train_scaled_full[\"Anomaly_IForest\"]}), 'Anomaly_IForest', batch_size=SAMPLE_FOR_SIL, n_batches=5, metric='euclidean')\n",
    "sil_lof= batched_silhouette(df_train_scaled_full.assign(**{ 'labels_lof': df_train_scaled_full[\"Anomaly_LOF\"]}), 'Anomaly_LOF', batch_size=SAMPLE_FOR_SIL, n_batches=5, metric='euclidean')\n",
    "\n",
    "print(\"Silhouette IForest (mean of batches):\", np.nanmean(sil_if))\n",
    "print(\"Silhouette LOF     (mean of batches):\", np.nanmean(sil_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabd6d8-e465-4e82-a8d8-8fdc0fca5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 10. Feature-level summaries & plots (KDEs etc)\n",
    "# -------------------------\n",
    "def feature_summary_and_plots(df_scaled_full, original_df, model_cols, features):\n",
    "    \"\"\"For each model in model_cols, produce feature summaries and KDEs (side-by-side).\"\"\"\n",
    "    summaries = {}\n",
    "    for model_col in model_cols:\n",
    "        summary = {}\n",
    "        summary['anomaly_rate'] = (df_scaled_full[model_col] == -1).mean() * 100\n",
    "        feat_stats = {}\n",
    "        for f in features:\n",
    "            # map back scaled -> original feature values roughly using the scaler inverse\n",
    "            # but we also can read original_df to get real values aligned: careful w/ dedup.\n",
    "            # For simplicity we compute stats from original_df filtered by mask aligned via index if same size.\n",
    "            mask = (df_scaled_full[model_col] == -1)\n",
    "            feat_stats[f] = {\n",
    "                'mean_normal': original_df.loc[~mask, f].mean() if f in original_df.columns else np.nan,\n",
    "                'mean_anomaly': original_df.loc[mask, f].mean() if f in original_df.columns else np.nan,\n",
    "                'median_normal': original_df.loc[~mask, f].median() if f in original_df.columns else np.nan,\n",
    "                'median_anomaly': original_df.loc[mask, f].median() if f in original_df.columns else np.nan,\n",
    "                'anomaly_count': mask.sum()\n",
    "            }\n",
    "        summary['feature_stats'] = feat_stats\n",
    "        summaries[model_col] = summary\n",
    "\n",
    "    # Plots: KDE side-by-side for each feature\n",
    "    for f in features:\n",
    "        fig, axes = plt.subplots(1, len(model_cols), figsize=(5 * len(model_cols), 4))\n",
    "        if len(model_cols) == 1:\n",
    "            axes = [axes]\n",
    "        for ax, model_col in zip(axes, model_cols):\n",
    "            mask_an = df_scaled_full[model_col] == -1\n",
    "            mask_no = df_scaled_full[model_col] == 1\n",
    "            # attempt to plot using original_df if possible to keep units\n",
    "            data_normal = original_df.loc[mask_no, f] if f in original_df.columns else df_scaled_full.loc[mask_no, f]\n",
    "            data_anom   = original_df.loc[mask_an, f] if f in original_df.columns else df_scaled_full.loc[mask_an, f]\n",
    "            if data_normal.dropna().shape[0] > 1:\n",
    "                sns.kdeplot(data_normal, ax=ax, label='Normal', fill=True, color='blue')\n",
    "            if data_anom.dropna().shape[0] > 1:\n",
    "                sns.kdeplot(data_anom, ax=ax, label='Anomaly', fill=True, color='red')\n",
    "            ax.set_title(f\"{model_col} - {f}\")\n",
    "            ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return summaries\n",
    "\n",
    "# Build summaries for train (use original df_train for real feature units)\n",
    "model_cols = [\"Anomaly_IForest\", \"Anomaly_LOF\"]\n",
    "train_summaries = feature_summary_and_plots(df_train_scaled_full, df_train.reset_index(drop=True), model_cols, [\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "print(\"Train summaries done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f972f2-dd12-4b0c-b930-6c6baeb8c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 11. If labels exist -> compute confusion & per-feature TP/FP/FN/TN\n",
    "# -------------------------\n",
    "def compute_confusion_and_feature_errors(original_df, df_scaled_full, model_col, label_col=LABEL_COL, features=NUM_FEATURES):\n",
    "    \"\"\"If ground truth exists, compute confusion and per-feature breakdown.\"\"\"\n",
    "    if label_col not in original_df.columns:\n",
    "        return None\n",
    "    # Normalize label: try to map -1/1 or 0/1 to 1=anomaly\n",
    "    y_true = original_df[label_col].copy().astype(int)\n",
    "    # If labels are 1 (normal) 0 (anomaly) detect and transform: assume anomalies are 1 when values {-1,1} present\n",
    "    if set(y_true.unique()) <= {0,1}:  # 0/1, assume 1=anomaly? ambiguous -> attempt to detect majority\n",
    "        # we assume 1 means anomaly only if anomalies are minority; otherwise try -1/1\n",
    "        pass\n",
    "    # We'll make y_true_binary such that 1=anomaly, 0=normal\n",
    "    if set(y_true.unique()) == {-1,1}:\n",
    "        y_true_bin = (y_true == -1).astype(int)\n",
    "    elif set(y_true.unique()) <= {0,1}:\n",
    "        # assume 1 == anomaly if anomaly labels are rare; otherwise ask user — but here we map 1 -> anomaly\n",
    "        y_true_bin = y_true.copy()\n",
    "    else:\n",
    "        # unknown encoding\n",
    "        y_true_bin = y_true.copy()\n",
    "\n",
    "    y_pred = (df_scaled_full[model_col] == -1).astype(int)\n",
    "    cm = confusion_matrix(y_true_bin[:len(y_pred)], y_pred)\n",
    "    report = classification_report(y_true_bin[:len(y_pred)], y_pred, digits=4)\n",
    "    # per-feature error rates: compute FP rate among samples where feature in top quantile, etc.\n",
    "    per_feature = {}\n",
    "    for feat in features:\n",
    "        if feat in original_df.columns:\n",
    "            q75 = original_df[feat].quantile(0.75)\n",
    "            high_mask = original_df[feat] >= q75\n",
    "            # in this high-value subgroup: compute precision/recall\n",
    "            if high_mask.sum() > 0:\n",
    "                y_t = y_true_bin[high_mask]\n",
    "                y_p = y_pred[high_mask][:len(y_t)]\n",
    "                prec, rec, f1, _ = precision_recall_fscore_support(y_t, y_p, average='binary', zero_division=0)\n",
    "            else:\n",
    "                prec, rec, f1 = np.nan, np.nan, np.nan\n",
    "            per_feature[feat] = {'q75_count': int(high_mask.sum()), 'precision_high': prec, 'recall_high': rec, 'f1_high': f1}\n",
    "    return {'confusion_matrix': cm, 'classification_report': report, 'per_feature_high': per_feature}\n",
    "\n",
    "# check for train labels\n",
    "train_conf = compute_confusion_and_feature_errors(df_train.reset_index(drop=True), df_train_scaled_full, 'Anomaly_IForest', LABEL_COL, features=[\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "if train_conf:\n",
    "    print(\"Confusion and feature-level errors (train):\")\n",
    "    print(train_conf['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8f251-e74e-40bd-a768-3da9b5c478d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 12. Permutation importance for feature relevance (IsolationForest)\n",
    "# -------------------------\n",
    "# Permutation importance needs a predict_proba-like continuous score. For IsolationForest we can use decision_function\n",
    "try:\n",
    "    iso_scores = iso_forest.decision_function(df_train_scaled_full[NUM_FEATURES])\n",
    "    r = permutation_importance(iso_forest, df_train_scaled_full[NUM_FEATURES], iso_scores, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    imp_df = pd.DataFrame({'feature': NUM_FEATURES, 'importance_mean': r.importances_mean, 'importance_std': r.importances_std})\n",
    "    imp_df.sort_values('importance_mean', ascending=False, inplace=True)\n",
    "    print(\"Permutation importances (IForest):\")\n",
    "    print(imp_df)\n",
    "except Exception as e:\n",
    "    print(\"Permutation importance failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207ac2b-152f-4354-aea5-8d106d888553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 13. Dotted summary plot: per-feature comparison (anomaly rates and (if available) proxy precision)\n",
    "# -------------------------\n",
    "def dotted_summary_plot(df_scaled_full, original_df, model_cols, features):\n",
    "    recs = []\n",
    "    for model_col in model_cols:\n",
    "        for f in features:\n",
    "            mask_an = df_scaled_full[model_col] == -1\n",
    "            # anomaly rate in top quantile for this feature\n",
    "            if f in original_df.columns:\n",
    "                q75 = original_df[f].quantile(0.75)\n",
    "                top = original_df[f] >= q75\n",
    "                # proportion of anomalies within top quantile:\n",
    "                prop_in_top = (mask_an[top.values]).sum() / max(1, top.sum())\n",
    "            else:\n",
    "                prop_in_top = np.nan\n",
    "            recs.append({'model': model_col, 'feature': f, 'anomaly_rate': (mask_an.mean()*100),'prop_top_q75': prop_in_top})\n",
    "    df_plot = pd.DataFrame(recs)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for model, g in df_plot.groupby('model'):\n",
    "        plt.scatter(g['feature'], g['anomaly_rate'], s=100, label=model)\n",
    "    plt.ylabel('Anomaly Rate (%)')\n",
    "    plt.title('Per-feature anomaly rates (by model)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return df_plot\n",
    "\n",
    "df_plot = dotted_summary_plot(df_train_scaled_full, df_train.reset_index(drop=True), model_cols, [\"BoxCox_Length\",\"Log_BRate\",\"Log_IATime\"])\n",
    "print(\"Dotted summary table:\")\n",
    "print(df_plot)\n",
    "\n",
    "# -------------------------\n",
    "# End - prints and small guidance\n",
    "# -------------------------\n",
    "print(\"Done. Key artifacts:\")\n",
    "print(f\" - df_train_scaled_full, df_test_scaled_full contain scaled features and model labels.\")\n",
    "print(\" - train_summaries contains per-model feature stats returned earlier.\")\n",
    "print(\" - df_plot contains the dotted-summary aggregated table.\")\n",
    "print(\"Next: inspect train_summaries and df_plot, examine KDEs and permutation importances.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
