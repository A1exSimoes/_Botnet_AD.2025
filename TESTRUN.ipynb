{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994fc6c9-da20-410f-8a83-e0546a1bbe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading small sample for feature discovery / quick stats...\n",
      "Processing full train CSV in chunks and building feature matrix (this may take time)...\n",
      " - processed train chunk 1, cumulative rows: 500000\n",
      " - processed train chunk 2, cumulative rows: 1000000\n",
      " - processed train chunk 3, cumulative rows: 1500000\n",
      " - processed train chunk 4, cumulative rows: 2000000\n",
      " - processed train chunk 5, cumulative rows: 2500000\n",
      " - processed train chunk 6, cumulative rows: 3000000\n",
      " - processed train chunk 7, cumulative rows: 3500000\n",
      " - processed train chunk 8, cumulative rows: 4000000\n",
      " - processed train chunk 9, cumulative rows: 4500000\n",
      " - processed train chunk 10, cumulative rows: 5000000\n",
      " - processed train chunk 11, cumulative rows: 5114514\n",
      "Total train rows (features): 5114514\n",
      "Processing test CSV in chunks and building feature matrix...\n",
      " - processed test chunk 1, cumulative rows: 500000\n",
      " - processed test chunk 2, cumulative rows: 1000000\n",
      " - processed test chunk 3, cumulative rows: 1500000\n",
      " - processed test chunk 4, cumulative rows: 2000000\n",
      " - processed test chunk 5, cumulative rows: 2500000\n",
      " - processed test chunk 6, cumulative rows: 3000000\n",
      " - processed test chunk 7, cumulative rows: 3500000\n",
      " - processed test chunk 8, cumulative rows: 4000000\n",
      " - processed test chunk 9, cumulative rows: 4500000\n",
      " - processed test chunk 10, cumulative rows: 5000000\n",
      " - processed test chunk 11, cumulative rows: 5500000\n",
      " - processed test chunk 12, cumulative rows: 6000000\n",
      " - processed test chunk 13, cumulative rows: 6500000\n",
      " - processed test chunk 14, cumulative rows: 7000000\n",
      " - processed test chunk 15, cumulative rows: 7500000\n",
      " - processed test chunk 16, cumulative rows: 8000000\n",
      " - processed test chunk 17, cumulative rows: 8500000\n",
      " - processed test chunk 18, cumulative rows: 9000000\n",
      " - processed test chunk 19, cumulative rows: 9388270\n",
      "Total test rows (features): 9388270\n",
      "Fitting RobustScaler on train features...\n",
      "Training IsolationForest (may use subsampling internally)...\n",
      "IsolationForest trained in 18.7s\n",
      "Fitting LOF on a sample of 200000 rows (LOF expensive to fit on full data).\n",
      "LOF (sample fit) trained in 13.3s\n",
      "Predicting IsolationForest on train (batched)...\n",
      "Predicting IsolationForest on test (batched)...\n",
      "Predicting LOF (novelty) on train (batched)...\n",
      "Predicting LOF (novelty) on test (batched)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 395\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal pipeline time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t_all\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0_all)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60.0\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 395\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 283\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    281\u001b[0m lof_train_preds \u001b[38;5;241m=\u001b[39m batched_lof_predict(lof, X_train_scaled, batch_size\u001b[38;5;241m=\u001b[39mBATCH_PRED)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting LOF (novelty) on test (batched)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m lof_test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_lof_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_PRED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# ISO/LOF mapping: for IsolationForest, predictions are {1: inlier, -1: outlier}\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# For convenience create boolean anomaly columns\u001b[39;00m\n\u001b[0;32m    287\u001b[0m iso_train_anom \u001b[38;5;241m=\u001b[39m (iso_train_preds \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 176\u001b[0m, in \u001b[0;36mbatched_lof_predict\u001b[1;34m(lof_model, X_df, batch_size)\u001b[0m\n\u001b[0;32m    174\u001b[0m     j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i \u001b[38;5;241m+\u001b[39m batch_size, n)\n\u001b[0;32m    175\u001b[0m     Xb \u001b[38;5;241m=\u001b[39m X_df\u001b[38;5;241m.\u001b[39miloc[i:j]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 176\u001b[0m     preds[i:j] \u001b[38;5;241m=\u001b[39m \u001b[43mlof_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:361\u001b[0m, in \u001b[0;36mLocalOutlierFactor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_check_novelty_predict)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict the labels (1 inlier, -1 outlier) of X according to LOF.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    **Only available for novelty detection (when novelty is set to True).**\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        Returns -1 for anomalies/outliers and +1 for inliers.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:383\u001b[0m, in \u001b[0;36mLocalOutlierFactor._predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     shifted_opposite_lof_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     is_inlier \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(shifted_opposite_lof_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    385\u001b[0m     is_inlier[shifted_opposite_lof_scores \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:431\u001b[0m, in \u001b[0;36mLocalOutlierFactor.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_check_novelty_decision_function)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    407\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shifted opposite of the Local Outlier Factor of X.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \n\u001b[0;32m    409\u001b[0m \u001b[38;5;124;03m    Bigger is better, i.e. large values correspond to inliers.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m        outliers, positive scores represent inliers.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_lof.py:476\u001b[0m, in \u001b[0;36mLocalOutlierFactor.score_samples\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    473\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    474\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 476\u001b[0m distances_X, neighbors_indices_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors_\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    481\u001b[0m     distances_X \u001b[38;5;241m=\u001b[39m distances_X\u001b[38;5;241m.\u001b[39mastype(X\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:923\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[0;32m    922\u001b[0m         )\n\u001b[1;32m--> 923\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import davies_bouldin_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "TRAIN_PATH = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Testing_file.csv'\n",
    "TEST_PATH = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Training_file.csv'\n",
    "\n",
    "CHUNKSIZE = 500_000                # rows to read per chunk\n",
    "SAMPLE_FOR_QUICK = 100_000         # small sample for quick plotting / silhouette\n",
    "SAMPLE_FOR_LOF_FIT = 200_000       # max rows to fit LOF (LOF expensive)\n",
    "PCA_BATCH = 100_000                # batch size for IncrementalPCA partial_fit\n",
    "SILHOUETTE_SAMPLE = 10_000         # silhouette sample size\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# IsolationForest config (tweakable)\n",
    "IF_N_ESTIMATORS = 450\n",
    "IF_MAX_SAMPLES = \"auto\"            # or integer <= n_samples, or float fraction\n",
    "IF_CONTAMINATION = \"auto\"\n",
    "\n",
    "# LOF config (we fit on a sample to limit memory/time)\n",
    "LOF_N_NEIGHBORS = 25\n",
    "ALGORITHM = \"kd_tree\" # ball_tree\n",
    "LOF_METRIC = \"chebyshev\" #euclidean / manhattan\n",
    "LOF_CONTAMINATION = \"auto\"\n",
    "\n",
    "\n",
    "OUTPUT_SUMMARY = \"unsupervised_summary.json\"\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def read_csv_in_chunks(path, chunksize=CHUNKSIZE, usecols=None):\n",
    "    \"\"\"Yield dataframe chunks from CSV.\"\"\"\n",
    "    it = pd.read_csv(path, low_memory=False, chunksize=chunksize, usecols=usecols, encoding='ISO-8859-1')\n",
    "    for chunk in it:\n",
    "        yield chunk\n",
    "\n",
    "def downcast_df(df):\n",
    "    \"\"\"Downcast numeric dtypes to save memory (float32 / int32)\"\"\"\n",
    "    for col in df.select_dtypes(include=[\"float64\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    for col in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def safe_concat(chunks):\n",
    "    \"\"\"Concatenate a list of small chunks into a single df (caller must be mindful of memory).\"\"\"\n",
    "    if not chunks:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(chunks, axis=0, ignore_index=True)\n",
    "\n",
    "def safe_boxcox(series):\n",
    "    arr = np.asarray(series.fillna(0.0).astype(float) + 1e-6)\n",
    "    try:\n",
    "        out, _ = boxcox(arr + 1e-6)\n",
    "        return out\n",
    "    except Exception:\n",
    "        return np.log1p(arr)\n",
    "# -------------------------\n",
    "# FEATURE ENGINEERING (vectorized)\n",
    "# -------------------------\n",
    "def build_features_vectorized(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    src = 'Source_enc' if 'Source_enc' in df.columns else 'Source'\n",
    "    dst = 'Destination_enc' if 'Destination_enc' in df.columns else 'Destination'\n",
    "    if 'Time' in df.columns:\n",
    "        df['Time'] = pd.to_numeric(df['Time'], errors='coerce').fillna(0.0)\n",
    "        df['Time_Diff'] = df.groupby(src)['Time'].diff().fillna(df.groupby(src)['Time'].transform('median')).fillna(0.0)\n",
    "    else:\n",
    "        df['Time_Diff'] = 0.0\n",
    "    def pkt_rate_series(s):\n",
    "        a = s.dropna().values\n",
    "        if a.size < 2:\n",
    "            return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "        q1, q3 = np.percentile(a, [25,75])\n",
    "        iqr = max(q3 - q1, 1e-9)\n",
    "        cl = np.clip(a, q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "        denom = cl.max() - cl.min()\n",
    "        if denom <= 1e-6:\n",
    "            rate = np.zeros_like(a)\n",
    "        else:\n",
    "            rate = np.full_like(a, cl.size / denom, dtype=float)\n",
    "        out = pd.Series(rate, index=s.dropna().index)\n",
    "        return out.reindex(s.index).fillna(0.0)\n",
    "    if 'Time' in df.columns:\n",
    "        df['Packet_Rate'] = df.groupby(src)['Time'].transform(lambda s: pkt_rate_series(s)).fillna(0.0)\n",
    "    else:\n",
    "        df['Packet_Rate'] = 0.0\n",
    "    df['Inter_Arrival_Time'] = df.groupby(src)['Time_Diff'].transform(lambda s: s.rolling(10, min_periods=1).mean()).fillna(0.0).clip(lower=1e-6)\n",
    "    df['Burst_Rate'] = np.where(df['Inter_Arrival_Time']>1e-6, 1.0/df['Inter_Arrival_Time'], 0.0)\n",
    "    df['Length'] = pd.to_numeric(df.get('Length', 0)).fillna(0)\n",
    "    df['Length_Mean'] = df.groupby(src)['Length'].transform('mean').fillna(df['Length'].mean() if 'Length' in df.columns else 0.0)\n",
    "    df['Length_Std']  = df.groupby(src)['Length'].transform('std').fillna(0.0)\n",
    "    df['Pkt_Per_Src'] = df.groupby(src)['Length'].transform('count').fillna(0).astype(int)\n",
    "    if 'Time' in df.columns:\n",
    "        df['Session_Dur_Src'] = df.groupby(src)['Time'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "        df['Session_Dur_Dst'] = df.groupby(dst)['Time'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "    else:\n",
    "        df['Session_Dur_Src'] = 0.0\n",
    "        df['Session_Dur_Dst'] = 0.0\n",
    "    df['Bytes_Per_Session_Src'] = df.groupby(src)['Length'].transform('sum').fillna(0.0)\n",
    "    df['Bytes_Per_Session_Dst'] = df.groupby(dst)['Length'].transform('sum').fillna(0.0)\n",
    "    df['Packets_Per_Session_Src'] = df['Pkt_Per_Src']\n",
    "    df['Packets_Per_Session_Dst'] = df.groupby(dst)['Length'].transform('count').fillna(0).astype(int)\n",
    "    df['Log_IATime'] = np.log1p(df['Inter_Arrival_Time'])\n",
    "    df['Log_BRate']  = np.log1p(df['Burst_Rate'].clip(lower=0))\n",
    "    df['BoxCox_Length'] = safe_boxcox(df['Length']) if 'Length' in df.columns else np.zeros(len(df))\n",
    "    df['BoxCox_PRate']  = safe_boxcox(df['Packet_Rate'])\n",
    "    df['Rate_to_Length'] = df['Burst_Rate'] / (df['BoxCox_Length'] + 1e-6)\n",
    "    df['IAT_to_Session'] = df['Inter_Arrival_Time'] / (df['Session_Dur_Src'] + 1e-6)\n",
    "    df['Burst_Variability'] = df['Burst_Rate'].rolling(5, min_periods=1).std().fillna(0.0).values\n",
    "    df['Pkt_Rate_Change']  = df['Packet_Rate'].diff().fillna(0.0).values\n",
    "    \n",
    "    features = ['Time_Diff','Log_IATime','Log_BRate',\n",
    "                'BoxCox_Length','BoxCox_PRate','Length_Mean',\n",
    "                'Length_Std','Pkt_Per_Src','Session_Dur_Src',\n",
    "                'Session_Dur_Dst','Rate_to_Length','IAT_to_Session',\n",
    "                'Burst_Variability','Pkt_Rate_Change',\n",
    "                'Bytes_Per_Session_Src','Packets_Per_Session_Src']\n",
    "\n",
    "    # Make sure each feature exists\n",
    "    for f in features:\n",
    "        if f not in df.columns:\n",
    "            df[f] = 0.0\n",
    "\n",
    "    # Downcast to save memory\n",
    "    df = downcast_df(df)\n",
    "\n",
    "    return df, features\n",
    "\n",
    "# -------------------------\n",
    "# UTILITY: batched predict\n",
    "# -------------------------\n",
    "def batched_predict(model, X_df, batch_size=100_000):\n",
    "    \"\"\"Predict in batches (works for models with predict supporting numpy arrays).\"\"\"\n",
    "    n = len(X_df)\n",
    "    preds = np.empty(n, dtype=object)\n",
    "    scores = None\n",
    "    # Some models provide predict_proba; we can try to capture it\n",
    "    has_proba = hasattr(model, \"predict_proba\")\n",
    "    if has_proba:\n",
    "        scores = np.zeros((n, len(model.classes_)))\n",
    "    for i in range(0, n, batch_size):\n",
    "        j = min(i + batch_size, n)\n",
    "        Xb = X_df.iloc[i:j].values\n",
    "        preds[i:j] = model.predict(Xb)\n",
    "        if has_proba:\n",
    "            scores[i:j, :] = model.predict_proba(Xb)\n",
    "    return preds, scores\n",
    "\n",
    "def batched_lof_predict(lof_model, X_df, batch_size=100_000):\n",
    "    \"\"\"LOF novelty predict in batches. LOF.predict returns 1 for inlier, -1 for outlier.\"\"\"\n",
    "    n = len(X_df)\n",
    "    preds = np.zeros(n, dtype=int)\n",
    "    for i in range(0, n, batch_size):\n",
    "        j = min(i + batch_size, n)\n",
    "        Xb = X_df.iloc[i:j].values\n",
    "        preds[i:j] = lof_model.predict(Xb)\n",
    "    return preds\n",
    "\n",
    "# -------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------\n",
    "def main():\n",
    "    t0_all = time.time()\n",
    "\n",
    "    # STEP 1: Read small sample (for feature set discovery & quick plots)\n",
    "    print(\"Reading small sample for feature discovery / quick stats...\")\n",
    "    sample_chunks = []\n",
    "    sample_rows = 0\n",
    "    for chunk in read_csv_in_chunks(TRAIN_PATH, chunksize=CHUNKSIZE):\n",
    "        sample_chunks.append(chunk)\n",
    "        sample_rows += len(chunk)\n",
    "        if sample_rows >= SAMPLE_FOR_QUICK:\n",
    "            break\n",
    "    if len(sample_chunks) == 0:\n",
    "        raise RuntimeError(\"No data found in TRAIN_PATH.\")\n",
    "    sample_df = safe_concat(sample_chunks).reset_index(drop=True)\n",
    "    sample_df, discovered_features = build_features_vectorized(sample_df)\n",
    "    #print(\"Discovered features:\", discovered_features)\n",
    "\n",
    "    # STEP 2: Load entire TRAIN (chunked), compute features, and collect (optionally sample)\n",
    "    print(\"Processing full train CSV in chunks and building feature matrix (this may take time)...\")\n",
    "    train_feature_frames = []\n",
    "    n_train_rows = 0\n",
    "    for idx, chunk in enumerate(read_csv_in_chunks(TRAIN_PATH, chunksize=CHUNKSIZE)):\n",
    "        chunk, _ = build_features_vectorized(chunk)\n",
    "        train_feature_frames.append(chunk[discovered_features])  # keep only feature columns\n",
    "        n_train_rows += len(chunk)\n",
    "        print(f\" - processed train chunk {idx+1}, cumulative rows: {n_train_rows}\")\n",
    "    df_train_feats = safe_concat(train_feature_frames)\n",
    "    print(f\"Total train rows (features): {len(df_train_feats)}\")\n",
    "\n",
    "    # STEP 3: Load entire TEST similarly\n",
    "    print(\"Processing test CSV in chunks and building feature matrix...\")\n",
    "    test_feature_frames = []\n",
    "    n_test_rows = 0\n",
    "    for idx, chunk in enumerate(read_csv_in_chunks(TEST_PATH, chunksize=CHUNKSIZE)):\n",
    "        chunk, _ = build_features_vectorized(chunk)\n",
    "        test_feature_frames.append(chunk[discovered_features])\n",
    "        n_test_rows += len(chunk)\n",
    "        print(f\" - processed test chunk {idx+1}, cumulative rows: {n_test_rows}\")\n",
    "    df_test_feats = safe_concat(test_feature_frames)\n",
    "    print(f\"Total test rows (features): {len(df_test_feats)}\")\n",
    "\n",
    "    # Optionally free memory used by chunk lists\n",
    "    train_feature_frames = None\n",
    "    test_feature_frames = None\n",
    "\n",
    "    # STEP 4: Scaling (RobustScaler fit on train)\n",
    "    print(\"Fitting RobustScaler on train features...\")\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(df_train_feats.values)   # may be large but required once\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(df_train_feats.values), columns=discovered_features)\n",
    "    X_test_scaled  = pd.DataFrame(scaler.transform(df_test_feats.values), columns=discovered_features)\n",
    "    # reduce dtype\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)\n",
    "    X_test_scaled  = X_test_scaled.astype(np.float32)\n",
    "\n",
    "    # STEP 5: Isolation Forest training\n",
    "    print(\"Training IsolationForest (may use subsampling internally)...\")\n",
    "    t0 = time.time()\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=IF_N_ESTIMATORS,\n",
    "        max_samples=IF_MAX_SAMPLES,\n",
    "        contamination=IF_CONTAMINATION,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,  # 0 for cleaner logs\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    iso.fit(X_train_scaled.values)\n",
    "    t1 = time.time()\n",
    "    print(f\"IsolationForest trained in {(t1-t0):.1f}s\")\n",
    "\n",
    "    # STEP 6: LOF fitting on a sample (LOF is costly) and then batched predict\n",
    "    lof_sample_n = min(SAMPLE_FOR_LOF_FIT, len(X_train_scaled))\n",
    "    if lof_sample_n < 1000:\n",
    "        lof_sample_n = min(1000, len(X_train_scaled))\n",
    "    print(f\"Fitting LOF on a sample of {lof_sample_n} rows (LOF expensive to fit on full data).\")\n",
    "    # sample train rows for LOF fit (stratified not applicable; we just sample randomly)\n",
    "    lof_fit_df = X_train_scaled.sample(n=lof_sample_n, random_state=RANDOM_STATE)\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=LOF_N_NEIGHBORS,\n",
    "        metric=LOF_METRIC,\n",
    "        algorithm=ALGORITHM,\n",
    "        contamination=LOF_CONTAMINATION,\n",
    "        novelty=True,   # allow predict on new data\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    lof.fit(lof_fit_df.values)   # fit on sample\n",
    "    t1 = time.time()\n",
    "    print(f\"LOF (sample fit) trained in {(t1-t0):.1f}s\")\n",
    "\n",
    "    # STEP 7: Predictions (batched) for both models on train & test\n",
    "    BATCH_PRED = 200_000\n",
    "    print(\"Predicting IsolationForest on train (batched)...\")\n",
    "    iso_train_preds, iso_train_scores = batched_predict(iso, X_train_scaled, batch_size=BATCH_PRED)\n",
    "    print(\"Predicting IsolationForest on test (batched)...\")\n",
    "    iso_test_preds, iso_test_scores = batched_predict(iso, X_test_scaled, batch_size=BATCH_PRED)\n",
    "\n",
    "    print(\"Predicting LOF (novelty) on train (batched)...\")\n",
    "    lof_train_preds = batched_lof_predict(lof, X_train_scaled, batch_size=BATCH_PRED)\n",
    "    print(\"Predicting LOF (novelty) on test (batched)...\")\n",
    "    lof_test_preds = batched_lof_predict(lof, X_test_scaled, batch_size=BATCH_PRED)\n",
    "\n",
    "    # ISO/LOF mapping: for IsolationForest, predictions are {1: inlier, -1: outlier}\n",
    "    # For convenience create boolean anomaly columns\n",
    "    iso_train_anom = (iso_train_preds == -1)\n",
    "    iso_test_anom  = (iso_test_preds == -1)\n",
    "    lof_train_anom = (lof_train_preds == -1)\n",
    "    lof_test_anom  = (lof_test_preds == -1)\n",
    "\n",
    "    # Agreement\n",
    "    train_agreement = np.mean(iso_train_anom == lof_train_anom) * 100.0\n",
    "    test_agreement  = np.mean(iso_test_anom == lof_test_anom) * 100.0\n",
    "\n",
    "    # Anomaly percentages\n",
    "    iso_train_pct = iso_train_anom.mean() * 100.0\n",
    "    iso_test_pct  = iso_test_anom.mean() * 100.0\n",
    "    lof_train_pct = lof_train_anom.mean() * 100.0\n",
    "    lof_test_pct  = lof_test_anom.mean() * 100.0\n",
    "\n",
    "    print(f\"IsolationForest train anomaly %: {iso_train_pct:.2f}\")\n",
    "    print(f\"IsolationForest test anomaly %:  {iso_test_pct:.2f}\")\n",
    "    print(f\"LOF train anomaly %: {lof_train_pct:.2f}\")\n",
    "    print(f\"LOF test anomaly %:  {lof_test_pct:.2f}\")\n",
    "    print(f\"Model agreement on train: {train_agreement:.2f}%\")\n",
    "    print(f\"Model agreement on test:  {test_agreement:.2f}%\")\n",
    "\n",
    "    # STEP 8: Davies–Bouldin Index (on a downsampled set to save time)\n",
    "    db_sample_n = min(SILHOUETTE_SAMPLE, len(X_train_scaled))\n",
    "    db_idx = np.random.RandomState(RANDOM_STATE).choice(len(X_train_scaled), size=db_sample_n, replace=False)\n",
    "    db_X = X_train_scaled.iloc[db_idx].values\n",
    "    # For DB index we need labels: use IsolationForest anomaly labels\n",
    "    labels_for_db = iso_train_anom[db_idx].astype(int)  # 0/1\n",
    "    \n",
    "    try:\n",
    "        db_score_if = davies_bouldin_score(db_X, labels_for_db)\n",
    "    except Exception:\n",
    "        db_score_if = float(\"nan\")\n",
    "    print(f\"Davies–Bouldin Index (sample, IsolationForest labels): {db_score_if:.4f}\")\n",
    "\n",
    "\n",
    "    # STEP 9: PCA visualization (IncrementalPCA for large sets)\n",
    "    # We'll compute 2-component PCA for plotting — use incremental partial_fit in batches\n",
    "    print(\"Computing IncrementalPCA (2 components) for visualization (train sample)...\")\n",
    "    ipca = IncrementalPCA(n_components=5)\n",
    "    # Fit on samples (we can partial_fit on chunks)\n",
    "    fit_iter = 0\n",
    "    for i in range(0, len(X_train_scaled), PCA_BATCH):\n",
    "        batch = X_train_scaled.iloc[i:i + PCA_BATCH].values\n",
    "        ipca.partial_fit(batch)\n",
    "        fit_iter += 1\n",
    "    # Transform (sample for plotting)\n",
    "    plot_sample_n = min(100_000, len(X_train_scaled))\n",
    "    plot_idx = np.random.RandomState(RANDOM_STATE).choice(len(X_train_scaled), size=plot_sample_n, replace=False)\n",
    "    plot_X = X_train_scaled.iloc[plot_idx].values\n",
    "    plot_proj = ipca.transform(plot_X)\n",
    "\n",
    "    # Prepare labels for plotting (0 normal / 1 anomaly)\n",
    "    plot_iso_labels = iso_train_anom[plot_idx].astype(int)\n",
    "    plot_lof_labels = lof_train_anom[plot_idx].astype(int)\n",
    "\n",
    "    # Scatter plot for IsolationForest\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(plot_proj[:, 0], plot_proj[:, 1], c=plot_iso_labels, cmap=\"coolwarm\", alpha=0.4, s=6)\n",
    "    plt.title(\"PCA scatter (IsolationForest labels) - train sample\")\n",
    "    plt.colorbar(label=\"Anomaly (1) / Inlier (0)\")\n",
    "\n",
    "    # Scatter plot for LOF\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(plot_proj[:, 0], plot_proj[:, 1], c=plot_lof_labels, cmap=\"coolwarm\", alpha=0.4, s=6)\n",
    "    plt.title(\"PCA scatter (LOF labels) - train sample\")\n",
    "    plt.colorbar(label=\"Anomaly (1) / Inlier (0)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # STEP 10: Feature distributions plots (histograms) for a small sample\n",
    "    small_plot_df = df_train_feats.sample(n=min(50_000, len(df_train_feats)), random_state=RANDOM_STATE)\n",
    "    for feat in discovered_features:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        sns.histplot(small_plot_df[feat].astype(float), bins=100, kde=True)\n",
    "        plt.title(f\"Distribution: {feat}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # STEP 11: Build summary and save\n",
    "    summary = {\n",
    "        \"n_train_rows\": len(X_train_scaled),\n",
    "        \"n_test_rows\": len(X_test_scaled),\n",
    "        \"IsolationForest\": {\n",
    "            \"train_anomaly_pct\": float(iso_train_pct),\n",
    "            \"test_anomaly_pct\": float(iso_test_pct)\n",
    "        },\n",
    "        \"LOF\": {\n",
    "            \"train_anomaly_pct\": float(lof_train_pct),\n",
    "            \"test_anomaly_pct\": float(lof_test_pct),\n",
    "            \"fit_sample_n\": int(lof_sample_n)\n",
    "        },\n",
    "        \"agreement\": {\n",
    "            \"train_pct\": float(train_agreement),\n",
    "            \"test_pct\": float(test_agreement)\n",
    "        },\n",
    "        \"db_index_IF\": float(db_score_if),\n",
    "        \"features\": discovered_features\n",
    "    }\n",
    "    with open(OUTPUT_SUMMARY, \"w\") as fh:\n",
    "        json.dump(summary, fh, indent=2)\n",
    "    print(\"Summary saved to\", OUTPUT_SUMMARY)\n",
    "\n",
    "    t_all = time.time()\n",
    "    print(f\"Total pipeline time: {(t_all - t0_all)/60.0:.2f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a443d1-a988-4620-a137-84614de6b3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
