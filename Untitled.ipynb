{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8d2a7-ca3a-4e95-8cbf-c1a0e423c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, warnings, os\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "import hdbscan\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG - adapt paths & sizes\n",
    "# -------------------------\n",
    "TRAIN_PATH = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Testing_file.csv'\n",
    "TEST_PATH  = r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Training_file\n",
    "SAMPLE_SIZE = 100_000                 # reduce if memory constrained\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# IForest / HDBSCAN / PCA params\n",
    "IFOREST_PARAMS = dict(n_estimators=250, contamination=0.05, max_samples=0.5, random_state=RANDOM_STATE)\n",
    "HDB_MIN_CLUSTER = 500\n",
    "HDB_MIN_SAMPLES = 10\n",
    "PCA_COMPONENTS = 3\n",
    "\n",
    "# RandomForest params (used for feature importance)\n",
    "RF_PARAMS = dict(n_estimators=300, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Features to use (will be ensured during build)\n",
    "NUM_FEATURES = [\n",
    "    \"Time_Diff\",\"Log_IATime\",\"Log_BRate\",\n",
    "    \"BoxCox_Length\",\"BoxCox_PRate\",\n",
    "    \"Length_Mean\",\"Length_Std\",\"Pkt_Per_Src\",\n",
    "    \"Session_Dur_Src\",\"Session_Dur_Dst\",\n",
    "    \"Rate_to_Length\",\"IAT_to_Session\"\n",
    "]\n",
    "\n",
    "OUTPUT_SUMMARY = \"detection_summary_extended.json\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def safe_boxcox(series):\n",
    "    arr = np.asarray(series.fillna(0.0).astype(float) + 1e-6)\n",
    "    try:\n",
    "        out, _ = boxcox(arr + 1e-6)\n",
    "        return out\n",
    "    except Exception:\n",
    "        return np.log1p(arr)\n",
    "\n",
    "def safe_sample(df, n, seed=RANDOM_STATE):\n",
    "    if n >= len(df):\n",
    "        return df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "def ensure_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def safe_pct(col):\n",
    "    return round(100.0 * np.mean(col), 2)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load CSVs\n",
    "# -------------------------\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "df_test  = pd.read_csv(TEST_PATH, low_memory=False)\n",
    "\n",
    "# Basic safe cleaning (drop rows missing critical fields)\n",
    "for df in (df_train, df_test):\n",
    "    for col in ['Time','Source','Destination','Length']:\n",
    "        if col in df.columns:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "    if 'Info' in df.columns:\n",
    "        df['Info'] = df['Info'].fillna(\"Unknown\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Safe combined encoding for categorical columns (vectorized)\n",
    "# -------------------------\n",
    "def safe_map(train, test, col):\n",
    "    # create mapping from union of values\n",
    "    if col not in train.columns or col not in test.columns:\n",
    "        # if missing, create dummy\n",
    "        train[col] = train.get(col, \"Unknown\").astype(str)\n",
    "        test[col] = test.get(col, \"Unknown\").astype(str)\n",
    "    all_vals = pd.concat([train[col].astype(str), test[col].astype(str)], axis=0)\n",
    "    uniques = pd.unique(all_vals)\n",
    "    mapping = {v: i for i,v in enumerate(uniques)}\n",
    "    train[col + \"_enc\"] = train[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
    "    test[col + \"_enc\"]  = test[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
    "    return mapping\n",
    "\n",
    "print(\"Encoding categorical columns (Protocol/Source/Destination) ...\")\n",
    "safe_map(df_train, df_test, 'Protocol')\n",
    "safe_map(df_train, df_test, 'Source')\n",
    "safe_map(df_train, df_test, 'Destination')\n",
    "\n",
    "# -------------------------\n",
    "# 3. Feature engineering (vectorized + group operations)\n",
    "# -------------------------\n",
    "def build_features(df):\n",
    "    df = df.copy()\n",
    "    src = 'Source_enc' if 'Source_enc' in df.columns else 'Source'\n",
    "    dst = 'Destination_enc' if 'Destination_enc' in df.columns else 'Destination'\n",
    "\n",
    "    # Time_Diff per source\n",
    "    if 'Time' in df.columns:\n",
    "        df['Time'] = pd.to_numeric(df['Time'], errors='coerce').fillna(0.0)\n",
    "        df['Time_Diff'] = df.groupby(src)['Time'].diff()\n",
    "        # fallback to median per source when NaN\n",
    "        df['Time_Diff'] = df['Time_Diff'].fillna(df.groupby(src)['Time'].transform('median')).fillna(0.0)\n",
    "    else:\n",
    "        df['Time_Diff'] = 0.0\n",
    "\n",
    "    # Packet_Rate per source (IQR robust)\n",
    "    def pkt_rate_series(s):\n",
    "        a = s.dropna().values\n",
    "        if a.size < 2:\n",
    "            return np.zeros_like(s)\n",
    "        # approximate packet rate per group using count / (range) robustified\n",
    "        q1, q3 = np.percentile(a, [25,75])\n",
    "        iqr = max(q3 - q1, 1e-9)\n",
    "        cl = np.clip(a, q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "        denom = cl.max() - cl.min()\n",
    "        rate = np.zeros_like(a)\n",
    "        if denom <= 1e-6:\n",
    "            rate[:] = 0.0\n",
    "        else:\n",
    "            rate[:] = cl.size / denom\n",
    "        # return aligned series sized as group index\n",
    "        out = pd.Series(rate, index=s.dropna().index)\n",
    "        return out.reindex(s.index).fillna(0.0)\n",
    "\n",
    "    if 'Time' in df.columns:\n",
    "        # compute per-group packet rate by applying on groups of 'Time'\n",
    "        df['Packet_Rate'] = df.groupby(src)['Time'].transform(lambda s: pkt_rate_series(s))\n",
    "        df['Packet_Rate'] = df['Packet_Rate'].fillna(0.0)\n",
    "    else:\n",
    "        df['Packet_Rate'] = 0.0\n",
    "\n",
    "    # Inter-arrival and burst features\n",
    "    df['Inter_Arrival_Time'] = df.groupby(src)['Time_Diff'].transform(lambda s: s.rolling(10, min_periods=1).mean()).fillna(0.0).clip(lower=1e-6)\n",
    "    df['Burst_Rate'] = np.where(df['Inter_Arrival_Time']>1e-6, 1.0/df['Inter_Arrival_Time'], 0.0)\n",
    "\n",
    "    # variability features\n",
    "    df['Length'] = pd.to_numeric(df.get('Length', 0)).fillna(0)\n",
    "    df['Length_Mean'] = df.groupby(src)['Length'].transform('mean').fillna(df['Length'].mean() if 'Length' in df.columns else 0.0)\n",
    "    df['Length_Std']  = df.groupby(src)['Length'].transform('std').fillna(0.0)\n",
    "    df['Pkt_Per_Src'] = df.groupby(src)['Length'].transform('count').fillna(0).astype(int)\n",
    "\n",
    "    # session durations\n",
    "    if 'Time' in df.columns:\n",
    "        df['Session_Dur_Src'] = df.groupby(src)['Time'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "        df['Session_Dur_Dst'] = df.groupby(dst)['Time'].transform(lambda x: x.max() - x.min()).fillna(0.0)\n",
    "    else:\n",
    "        df['Session_Dur_Src'] = 0.0\n",
    "        df['Session_Dur_Dst'] = 0.0\n",
    "\n",
    "    # transforms\n",
    "    df['Log_IATime'] = np.log1p(df['Inter_Arrival_Time'])\n",
    "    df['Log_BRate']  = np.log1p(df['Burst_Rate'].clip(lower=0))\n",
    "\n",
    "    df['BoxCox_Length'] = safe_boxcox(df['Length']) if 'Length' in df.columns else np.zeros(len(df))\n",
    "    df['BoxCox_PRate']  = safe_boxcox(df['Packet_Rate'])\n",
    "\n",
    "    df['Rate_to_Length'] = df['Burst_Rate'] / (df['BoxCox_Length'] + 1e-6)\n",
    "    df['IAT_to_Session'] = df['Inter_Arrival_Time'] / (df['Session_Dur_Src'] + 1e-6)\n",
    "\n",
    "    # ensure numeric features present\n",
    "    df = ensure_numeric(df, NUM_FEATURES)\n",
    "    return df\n",
    "\n",
    "print(\"Building features...\")\n",
    "df_train = build_features(df_train)\n",
    "df_test  = build_features(df_test)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Add temporal/variability features requested\n",
    "# -------------------------\n",
    "for df in (df_train, df_test):\n",
    "    df['Burst_Variability'] = df['Burst_Rate'].rolling(5, min_periods=1).std().fillna(0.0).values\n",
    "    df['Pkt_Rate_Change']  = df['Packet_Rate'].diff().fillna(0.0).values\n",
    "\n",
    "# update feature list if new added\n",
    "for f in ['Burst_Variability','Pkt_Rate_Change']:\n",
    "    if f not in NUM_FEATURES:\n",
    "        NUM_FEATURES.append(f)\n",
    "\n",
    "print(f\"NUM_FEATURES used: {NUM_FEATURES}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Sampling for memory (safe)\n",
    "# -------------------------\n",
    "print(f\"Sampling up to {SAMPLE_SIZE} rows for train/test (reduce SAMPLE_SIZE to lower memory).\")\n",
    "df_train_sample = safe_sample(df_train, SAMPLE_SIZE)\n",
    "df_test_sample  = safe_sample(df_test, SAMPLE_SIZE)\n",
    "\n",
    "# -------------------------\n",
    "# 6. Scaling (fit on train sample and transform both)\n",
    "# -------------------------\n",
    "print(\"Scaling numeric features (RobustScaler)...\")\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(df_train_sample[NUM_FEATURES])\n",
    "X_train = pd.DataFrame(scaler.transform(df_train_sample[NUM_FEATURES]), columns=NUM_FEATURES, index=df_train_sample.index)\n",
    "X_test  = pd.DataFrame(scaler.transform(df_test_sample[NUM_FEATURES]),  columns=NUM_FEATURES, index=df_test_sample.index)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Isolation Forest\n",
    "# -------------------------\n",
    "print(\"Training IsolationForest...\")\n",
    "iso = IsolationForest(**IFOREST_PARAMS)\n",
    "t0 = time.time()\n",
    "iso.fit(X_train)\n",
    "t1 = time.time()\n",
    "print(f\"IForest trained in {(t1-t0):.1f}s\")\n",
    "\n",
    "df_train_sample['Anomaly_IForest'] = iso.predict(X_train)   # -1 anomaly, 1 normal\n",
    "df_test_sample['Anomaly_IForest']  = iso.predict(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# 8. Global PCA + HDBSCAN (fit PCA once on train)\n",
    "# -------------------------\n",
    "print(\"PCA reduction and HDBSCAN clustering (global PCA fit on train sample)...\")\n",
    "pca = PCA(n_components=min(PCA_COMPONENTS, len(NUM_FEATURES)), random_state=RANDOM_STATE)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca  = pca.transform(X_test)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=HDB_MIN_CLUSTER, min_samples=HDB_MIN_SAMPLES,\n",
    "                            metric='euclidean', cluster_selection_method='eom', core_dist_n_jobs=-1)\n",
    "t0 = time.time()\n",
    "train_labels = clusterer.fit_predict(X_train_pca)\n",
    "t1 = time.time()\n",
    "print(f\"HDBSCAN on train done in {(t1-t0):.1f}s. Clusters found (excl noise): {len(set(train_labels)) - (1 if -1 in train_labels else 0)}\")\n",
    "df_train_sample['HDBSCAN_Label'] = train_labels\n",
    "df_train_sample['HDBSCAN_IsNoise'] = (train_labels == -1).astype(int)\n",
    "\n",
    "# approximate_predict for test\n",
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, X_test_pca)\n",
    "df_test_sample['HDBSCAN_Label'] = test_labels\n",
    "df_test_sample['HDBSCAN_IsNoise'] = (test_labels == -1).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 9. Consensus anomaly mask\n",
    "# -------------------------\n",
    "df_train_sample['Consensus_Anomaly'] = (((df_train_sample['Anomaly_IForest'] == -1) & (df_train_sample['HDBSCAN_IsNoise'] == 1))).astype(int)\n",
    "df_test_sample['Consensus_Anomaly']  = (((df_test_sample['Anomaly_IForest'] == -1) & (df_test_sample['HDBSCAN_IsNoise'] == 1))).astype(int)\n",
    "\n",
    "# -------------------------\n",
    "# 10. Percentiles & botnet profiles (Neris/Virut/Zeus/Generic)\n",
    "# -------------------------\n",
    "print(\"Computing percentiles for profile generation...\")\n",
    "percentiles = [5,10,30,50,70,80,90,95]\n",
    "p = {}\n",
    "features_for_percentiles = [\"Pkt_Per_Src\",\"Burst_Rate\",\"Session_Dur_Src\",\"Inter_Arrival_Time\",\"BoxCox_Length\",\"Rate_to_Length\",\"IAT_to_Session\",\"Burst_Variability\",\"Pkt_Rate_Change\"]\n",
    "\n",
    "for feat in features_for_percentiles:\n",
    "    if feat in df_train_sample.columns:\n",
    "        vals = df_train_sample[feat].dropna().values\n",
    "        if vals.size == 0:\n",
    "            p[feat] = {perc: 0.0 for perc in percentiles}\n",
    "        else:\n",
    "            q = np.percentile(vals, percentiles)\n",
    "            p[feat] = {perc: float(x) for perc, x in zip(percentiles, q)}\n",
    "    else:\n",
    "        p[feat] = {perc: 0.0 for perc in percentiles}\n",
    "\n",
    "def pp(feat, perc):\n",
    "    return p.get(feat, {}).get(perc, 0.0)\n",
    "\n",
    "# Revised profiles (more forgiving bands to increase recall)\n",
    "botnet_profiles = {\n",
    "    \"Neris\": {\n",
    "        \"Pkt_Per_Src\": (pp(\"Pkt_Per_Src\", 70), np.inf),\n",
    "        \"Rate_to_Length\": (pp(\"Rate_to_Length\", 50), np.inf),\n",
    "        \"Burst_Rate\": (pp(\"Burst_Rate\", 40), np.inf)\n",
    "    },\n",
    "    \"Virut\": {\n",
    "        \"BoxCox_Length\": (pp(\"BoxCox_Length\", 60), np.inf),\n",
    "        \"Burst_Rate\": (pp(\"Burst_Rate\", 40), np.inf)\n",
    "    },\n",
    "    \"Zeus\": {\n",
    "        \"Inter_Arrival_Time\": (pp(\"Inter_Arrival_Time\", 5), pp(\"Inter_Arrival_Time\", 60)),\n",
    "        \"IAT_to_Session\": (pp(\"IAT_to_Session\", 10), pp(\"IAT_to_Session\", 90)),\n",
    "        \"Burst_Variability\": (pp(\"Burst_Variability\", 5), pp(\"Burst_Variability\", 80))\n",
    "    },\n",
    "    \"GenericBotnet\": {\n",
    "        \"Pkt_Rate_Change\": (pp(\"Pkt_Rate_Change\", 70), np.inf),\n",
    "        \"Burst_Variability\": (pp(\"Burst_Variability\", 70), np.inf)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Botnet profiles defined (percentile-driven).\")\n",
    "\n",
    "# Score-based profile matcher - partial matches allowed\n",
    "def score_profiles(row, profiles, min_matches=1):\n",
    "    scores = {}\n",
    "    for name, rules in profiles.items():\n",
    "        matches = 0\n",
    "        total = len(rules)\n",
    "        for feat, (low, high) in rules.items():\n",
    "            val = row.get(feat, np.nan)\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            if val >= low and val <= high:\n",
    "                matches += 1\n",
    "        scores[name] = (matches, total)\n",
    "    # choose best by (matches, total) and require at least min_matches\n",
    "    best = max(scores.items(), key=lambda kv: (kv[1][0], kv[1][1]))\n",
    "    if best[1][0] >= min_matches and best[1][0] > 0:\n",
    "        return best[0]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply profile matching to consensus anomalies only\n",
    "for df_s in (df_train_sample, df_test_sample):\n",
    "    df_s['Pred_Botnet_Profile'] = \"Normal\"\n",
    "    mask = df_s['Consensus_Anomaly'] == 1\n",
    "    if mask.any():\n",
    "        df_s.loc[mask, 'Pred_Botnet_Profile'] = df_s.loc[mask].apply(lambda r: score_profiles(r, botnet_profiles, min_matches=1), axis=1)\n",
    "        # fallback: if still Unknown but consensus anomaly -> GenericBotnet\n",
    "        fallback = (df_s['Pred_Botnet_Profile'] == \"Unknown\") & (df_s['Consensus_Anomaly'] == 1)\n",
    "        df_s.loc[fallback, 'Pred_Botnet_Profile'] = \"GenericBotnet\"\n",
    "\n",
    "print(\"Profile assignment done.\")\n",
    "print(\"Train profile counts:\", df_train_sample['Pred_Botnet_Profile'].value_counts().to_dict())\n",
    "print(\"Test profile counts: \", df_test_sample['Pred_Botnet_Profile'].value_counts().to_dict())\n",
    "\n",
    "# -------------------------\n",
    "# 11. Prepare supervised dataset (pseudo-labels)\n",
    "# -------------------------\n",
    "def prepare_rf_df(df_s):\n",
    "    dfc = df_s.copy()\n",
    "    dfc['RF_Label'] = dfc['Pred_Botnet_Profile'].fillna(\"Unknown\")\n",
    "    return dfc\n",
    "\n",
    "rf_train_df = prepare_rf_df(df_train_sample)\n",
    "rf_test_df  = prepare_rf_df(df_test_sample)\n",
    "\n",
    "# Build training set: keep known profiles and sample normals to balance\n",
    "anom_known = rf_train_df[rf_train_df['RF_Label'] != \"Unknown\"]\n",
    "normals = rf_train_df[rf_train_df['RF_Label'] == \"Normal\"]\n",
    "\n",
    "if len(anom_known) == 0:\n",
    "    print(\"No known anomalies to train supervised models. Skipping supervised stage.\")\n",
    "    do_supervised = False\n",
    "else:\n",
    "    do_supervised = True\n",
    "    # target: sample normals to not dwarf anomalies\n",
    "    class_counts = anom_known['RF_Label'].value_counts()\n",
    "    min_non_norm = class_counts.min()\n",
    "    n_normals_keep = min(len(normals), int(min_non_norm * max(3, len(class_counts))))\n",
    "    normals_sampled = normals.sample(n=n_normals_keep, random_state=RANDOM_STATE) if len(normals)>0 else pd.DataFrame()\n",
    "    rf_train_ready = pd.concat([anom_known, normals_sampled]).sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    rf_train_ready = rf_train_ready[rf_train_ready['RF_Label'] != \"Unknown\"]\n",
    "    print(\"Supervised training distribution:\", Counter(rf_train_ready['RF_Label']))\n",
    "\n",
    "# -------------------------\n",
    "# 12. Train classifiers (RF, LinearSVC, RBF SVM, Logistic) and ensemble\n",
    "# -------------------------\n",
    "models = {}\n",
    "feat_imp = pd.Series(0, index=NUM_FEATURES)\n",
    "\n",
    "if do_supervised and len(rf_train_ready) > 10:\n",
    "    X_train_sup = rf_train_ready[NUM_FEATURES].fillna(0.0)\n",
    "    y_train_sup = rf_train_ready['RF_Label']\n",
    "\n",
    "    # Try SMOTE to handle class imbalance (may fail on tiny classes)\n",
    "    try:\n",
    "        sm = SMOTE(random_state=RANDOM_STATE)\n",
    "        X_bal, y_bal = sm.fit_resample(X_train_sup, y_train_sup)\n",
    "        print(\"SMOTE applied: balanced shape:\", X_bal.shape)\n",
    "    except Exception as e:\n",
    "        print(\"SMOTE failed (fall back to original):\", e)\n",
    "        X_bal, y_bal = X_train_sup, y_train_sup\n",
    "\n",
    "    # Models\n",
    "    rf = RandomForestClassifier(**RF_PARAMS)\n",
    "    lsvc = LinearSVC(C=0.5, class_weight='balanced', max_iter=5000, random_state=RANDOM_STATE)\n",
    "    rbf_svc = SVC(C=1.0, kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "    logreg = LogisticRegression(max_iter=5000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit RF\n",
    "    print(\"Training RandomForest...\")\n",
    "    t0 = time.time()\n",
    "    rf.fit(X_bal, y_bal)\n",
    "    t1 = time.time()\n",
    "    print(f\"RF trained in {(t1-t0):.1f}s\")\n",
    "    models['RandomForest'] = rf\n",
    "    feat_imp = pd.Series(rf.feature_importances_, index=NUM_FEATURES).sort_values(ascending=False)\n",
    "\n",
    "    # Fit LinearSVC\n",
    "    print(\"Training LinearSVC...\")\n",
    "    t0 = time.time()\n",
    "    lsvc.fit(X_bal, y_bal)\n",
    "    t1 = time.time()\n",
    "    print(f\"LinearSVC trained in {(t1-t0):.1f}s\")\n",
    "    models['LinearSVC'] = lsvc\n",
    "\n",
    "    # Fit RBF SVM (may be slow)\n",
    "    try:\n",
    "        print(\"Training RBF SVC (may be slow)...\")\n",
    "        t0 = time.time()\n",
    "        rbf_svc.fit(X_bal, y_bal)\n",
    "        t1 = time.time()\n",
    "        print(f\"RBF SVC trained in {(t1-t0):.1f}s\")\n",
    "        models['RBF_SVC'] = rbf_svc\n",
    "    except Exception as e:\n",
    "        print(\"RBF SVC training failed/too slow; skipping:\", e)\n",
    "\n",
    "    # Fit Logistic Regression\n",
    "    print(\"Training LogisticRegression...\")\n",
    "    t0 = time.time()\n",
    "    logreg.fit(X_bal, y_bal)\n",
    "    t1 = time.time()\n",
    "    print(f\"LogisticRegression trained in {(t1-t0):.1f}s\")\n",
    "    models['LogisticRegression'] = logreg\n",
    "\n",
    "    # Voting ensemble (soft) using available models\n",
    "    estimators = []\n",
    "    estimators.append(('rf', rf))\n",
    "    estimators.append(('lsvc', lsvc))\n",
    "    if 'RBF_SVC' in models:\n",
    "        estimators.append(('rbf', models['RBF_SVC']))\n",
    "    estimators.append(('lr', logreg))\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting='soft', weights=[3,1,1,1] if 'RBF_SVC' in models else [3,1,1], n_jobs=-1)\n",
    "    print(\"Training Voting Ensemble...\")\n",
    "    t0 = time.time()\n",
    "    ensemble.fit(X_bal, y_bal)\n",
    "    t1 = time.time()\n",
    "    print(f\"Ensemble trained in {(t1-t0):.1f}s\")\n",
    "    models['Ensemble'] = ensemble\n",
    "\n",
    "    # Prepare test supervised set (keep Unknown as label if present)\n",
    "    X_test_sup = rf_test_df[NUM_FEATURES].fillna(0.0)\n",
    "    y_test_sup = rf_test_df['RF_Label']\n",
    "\n",
    "    # Predictions and reports\n",
    "    print(\"\\nRandomForest classification report (test sample):\")\n",
    "    y_pred_rf = rf.predict(X_test_sup)\n",
    "    print(classification_report(y_test_sup, y_pred_rf, zero_division=0))\n",
    "\n",
    "    print(\"\\nLinearSVC classification report (test sample):\")\n",
    "    y_pred_lsvc = lsvc.predict(X_test_sup)\n",
    "    print(classification_report(y_test_sup, y_pred_lsvc, zero_division=0))\n",
    "\n",
    "    if 'RBF_SVC' in models:\n",
    "        print(\"\\nRBF SVC classification report (test sample):\")\n",
    "        y_pred_rbf = models['RBF_SVC'].predict(X_test_sup)\n",
    "        print(classification_report(y_test_sup, y_pred_rbf, zero_division=0))\n",
    "\n",
    "    print(\"\\nLogisticRegression classification report (test sample):\")\n",
    "    y_pred_log = logreg.predict(X_test_sup)\n",
    "    print(classification_report(y_test_sup, y_pred_log, zero_division=0))\n",
    "\n",
    "    print(\"\\nEnsemble classification report (test sample):\")\n",
    "    y_pred_ens = ensemble.predict(X_test_sup)\n",
    "    print(classification_report(y_test_sup, y_pred_ens, zero_division=0))\n",
    "else:\n",
    "    print(\"Skipping supervised training due to insufficient pseudo-labeled anomalies.\")\n",
    "    models = {}\n",
    "\n",
    "# -------------------------\n",
    "# 13. Metrics & summary\n",
    "# -------------------------\n",
    "summary = {\n",
    "    \"IForest_train_pct\": safe_pct(df_train_sample['Anomaly_IForest'] == -1),\n",
    "    \"IForest_test_pct\": safe_pct(df_test_sample['Anomaly_IForest'] == -1),\n",
    "    \"HDBSCAN_train_pct\": safe_pct(df_train_sample['HDBSCAN_IsNoise'] == 1),\n",
    "    \"HDBSCAN_test_pct\": safe_pct(df_test_sample['HDBSCAN_IsNoise'] == 1),\n",
    "    \"Consensus_train_pct\": safe_pct(df_train_sample['Consensus_Anomaly'] == 1),\n",
    "    \"Consensus_test_pct\": safe_pct(df_test_sample['Consensus_Anomaly'] == 1),\n",
    "    \"Agreement_train_pct\": safe_pct(((df_train_sample['Anomaly_IForest'] == -1).astype(int) == df_train_sample['HDBSCAN_IsNoise'])),\n",
    "    \"Agreement_test_pct\": safe_pct(((df_test_sample['Anomaly_IForest'] == -1).astype(int) == df_test_sample['HDBSCAN_IsNoise'])),\n",
    "    \"Silhouette_IF_train\": None,\n",
    "    \"Silhouette_HDB_train\": None\n",
    "}\n",
    "\n",
    "# silhouette safety\n",
    "try:\n",
    "    if len(np.unique((df_train_sample['Anomaly_IForest'] == -1).astype(int))) > 1:\n",
    "        summary[\"Silhouette_IF_train\"] = round(silhouette_score(X_train, (df_train_sample['Anomaly_IForest'] == -1).astype(int)), 4)\n",
    "    if len(np.unique(df_train_sample['HDBSCAN_IsNoise'])) > 1:\n",
    "        summary[\"Silhouette_HDB_train\"] = round(silhouette_score(X_train, df_train_sample['HDBSCAN_IsNoise']), 4)\n",
    "except Exception as e:\n",
    "    print(\"Silhouette calc failed:\", e)\n",
    "\n",
    "print(\"=== SUMMARY ===\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"Top predicted botnet profiles (test sample):\")\n",
    "print(df_test_sample['Pred_Botnet_Profile'].value_counts().head(10))\n",
    "\n",
    "# -------------------------\n",
    "# 14. Diagnostics plots\n",
    "# -------------------------\n",
    "# Feature importances\n",
    "if 'RandomForest' in models:\n",
    "    feat_imp = pd.Series(models['RandomForest'].feature_importances_, index=NUM_FEATURES).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    feat_imp.head(12).plot(kind='barh')\n",
    "    plt.title(\"RF Top Features\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrix for ensemble if exists, else RF\n",
    "cm_model = 'Ensemble' if 'Ensemble' in models else ('RandomForest' if 'RandomForest' in models else None)\n",
    "if cm_model and do_supervised:\n",
    "    preds = models[cm_model].predict(X_test_sup)\n",
    "    labels = sorted(list(pd.unique(y_test_sup)))\n",
    "    cm = confusion_matrix(y_test_sup, preds, labels=labels)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion matrix - {cm_model}\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# KDE plots per feature (consensus anomaly vs normal)\n",
    "kde_feats = [\"Pkt_Per_Src\",\"Burst_Rate\",\"Inter_Arrival_Time\",\"BoxCox_Length\",\"Burst_Variability\"]\n",
    "for f in kde_feats:\n",
    "    if f in df_test_sample.columns:\n",
    "        plt.figure(figsize=(7,4))\n",
    "        sns.kdeplot(df_test_sample.loc[df_test_sample['Consensus_Anomaly']==0, f], label=\"Normal (consensus)\", fill=True)\n",
    "        sns.kdeplot(df_test_sample.loc[df_test_sample['Consensus_Anomaly']==1, f], label=\"Consensus Anom\", fill=True)\n",
    "        plt.title(f\"KDE Test Sample - {f}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# 15. Save JSON summary + top counts\n",
    "# -------------------------\n",
    "out = {\n",
    "    \"summary\": summary,\n",
    "    \"top_botnet_profiles_test\": df_test_sample['Pred_Botnet_Profile'].value_counts().head(20).to_dict(),\n",
    "    \"rf_feature_importances\": feat_imp.head(20).to_dict() if 'RandomForest' in models else {}\n",
    "}\n",
    "with open(OUTPUT_SUMMARY, \"w\") as fh:\n",
    "    json.dump(out, fh, indent=2)\n",
    "\n",
    "print(f\"Saved summary to {OUTPUT_SUMMARY}\")\n",
    "print(\"Pipeline finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
