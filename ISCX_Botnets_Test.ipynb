{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ef5ef2-e03b-45e3-9a05-f6d17d735229",
   "metadata": {},
   "source": [
    "## ISCX *Botnet* 2014 *Dataset*\n",
    "\n",
    "Este *dataset* foi retirado da página da unb.ca (University of New Brunswick): https://www.unb.ca/cic/datasets/botnet.html\n",
    "\n",
    "A informação que podemos obter é que se trata de uma junção de vários *datasets* sendo eles:\n",
    "- ISOT Dataset\n",
    "- ISCX 2012 IDS Dataset\n",
    "- Botnet traffic generated by the Malware Capture Facility Project\n",
    "\n",
    "### Informações do autor: \n",
    "O *dataset* de treino é composto por 43.92% de dados malignos e o *dataset* de teste é composto por 44.97% de dados malignos.\n",
    "\n",
    "---\n",
    "\n",
    "**No *dataset* de treino:**\n",
    "\n",
    "Botnet name | Type | Portion of flows in dataset\n",
    "\n",
    "    Neris                | IRC  | 21159  (12%)\n",
    "    Rbot                 | IRC  | 39316  (22%)\n",
    "    Virut                | HTTP | 1638   (0.94 %)\n",
    "    NSIS                 | P2P  | 4336   (2.48%)\n",
    "    SMTP Spam            | P2P  | 11296  (6.48%)\n",
    "    Zeus                 | P2P  | 31     (0.01%)\n",
    "    Zeus control (C & C) | P2P  | 20     (0.01%)\n",
    "\n",
    "--- \n",
    "\n",
    "**No *dataset* de teste:**\n",
    "\n",
    "Botnet name | Type | Portion of flows in dataset\n",
    "\n",
    "    Neris              | IRC  | 25967 (5.67%)\n",
    "    Rbot               | IRC  | 83    (0.018%)\n",
    "    Menti              | IRC  | 2878  (0.62%)\n",
    "    Sogou              | HTTP | 89    (0.019%)\n",
    "    Murlo              | IRC  | 4881  (1.06%)\n",
    "    Virut              | HTTP | 58576 (12.80%)\n",
    "    NSIS               | P2P  | 757   (0.165%)\n",
    "    Zeus               | P2P  | 502   (0.109%)\n",
    "    SMTP Spam          | P2P  | 21633 (4.72%)\n",
    "    UDP Storm          | P2P  | 44062 (9.63%)\n",
    "    Tbot               | IRC  | 1296  (0.283%)\n",
    "    Zero Access        | P2P  | 1011  (0.221%)\n",
    "    Weasel             | P2P  | 42313 (9.25%)\n",
    "    Smoke Bot          | P2P  | 78    (0.017%)\n",
    "    Zeus Control (C&C) | P2P  | 31    (0.006%)\n",
    "    ISCX IRC bot       | P2P  | 1816  (0.387%)\n",
    "\n",
    "---\n",
    "\n",
    "**IPs malignos:**\n",
    "\n",
    "    IRC\n",
    "        192.168.2.112 -> 131.202.243.84\n",
    "        192.168.5.122 -> 198.164.30.2\n",
    "        192.168.2.110 -> 192.168.5.122\n",
    "        192.168.4.118 -> 192.168.5.122\n",
    "        192.168.2.113 -> 192.168.5.122\n",
    "        192.168.1.103 -> 192.168.5.122\n",
    "        192.168.4.120 -> 192.168.5.122\n",
    "        192.168.2.112 -> 192.168.2.110\n",
    "        192.168.2.112 -> 192.168.4.120\n",
    "        192.168.2.112 -> 192.168.1.103\n",
    "        192.168.2.112 -> 192.168.2.113\n",
    "        192.168.2.112 -> 192.168.4.118\n",
    "        192.168.2.112 -> 192.168.2.109\n",
    "        192.168.2.112 -> 192.168.2.105\n",
    "        192.168.1.105 -> 192.168.5.122\n",
    "        \n",
    "    Neris: 147.32.84.180\n",
    "    RBot: 147.32.84.170\n",
    "    Menti: 147.32.84.150\n",
    "    Sogou: 147.32.84.140\n",
    "    Murlo: 147.32.84.130\n",
    "    Virut: 147.32.84.160\n",
    "    IRCbot and black hole1: 10.0.2.15\n",
    "    Black hole 2: 192.168.106.141\n",
    "    Black hole 3: 192.168.106.131\n",
    "    TBot: 172.16.253.130, 172.16.253.131, 172.16.253.129, 172.16.253.240\n",
    "    Weasel: Botmaster IP: 74.78.117.238; Bot IP: 158.65.110.24\n",
    "    Zeus (zeus sample 1 and 2 and 3, bin_zeus): 192.168.3.35, 192.168.3.25, 192.168.3.65, 172.29.0.116\n",
    "    Osx_trojan: 172.29.0.109\n",
    "    Zero access (zero access 1 and 2): 172.16.253.132, 192.168.248.165\n",
    "    Smoke bot: 10.37.130.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50b551-cd5f-4d3d-9f43-3b6b7cbfd694",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e976481-16a4-4a6b-ad3f-2280cd7ec8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor \n",
    "# Nova adição para feature engineering \n",
    "from scipy.stats import boxcox\n",
    "# Adição - métrica de avaliação de resultados Silhouette Score\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b0ebf-a4d5-4989-afcd-a675f89a4843",
   "metadata": {},
   "source": [
    "### Leitura de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074166e-7466-4a7c-baf7-2582b59944b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar datasets\n",
    "df_train = pd.read_csv(r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Testing_file.csv',\n",
    "                       encoding='ISO-8859-1')\n",
    "\n",
    "df_test = pd.read_csv(r'X:\\Dissertacao\\python_projects\\dataset\\ISCX-Bot-2014\\ISCX_csv\\Training_file.csv',\n",
    "                      encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7989680-2dfd-47f8-8ab3-4b29cdf3bee9",
   "metadata": {},
   "source": [
    "### Exploração dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddc99d-2824-43f3-ae6b-fa2aacb936c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d311ab-98bf-4a04-a5e8-7e21c8bce048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71882a0c-349a-4141-aa21-e84e55a52db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train (nulos):\\n\",df_train.isnull().sum())  \n",
    "print(\"Test (nulos):\\n\",df_test.isnull().sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac46b22-e529-4eb5-8021-3d6e7989a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['Time'].describe())  \n",
    "print(\"Valores de 'Time' negativos: \", (df_train['Time'] < 0).sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48206a-2bc8-45d6-bd5d-cc4a4c68ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['Time'].describe())  \n",
    "print(\"Valores de 'Time' negativos: \", (df_test['Time'] < 0).sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef005a0-b310-4c68-b2b5-d2d396df72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Protocolos em Train:\")\n",
    "print(df_train[\"Protocol\"].unique())\n",
    "# ________________________________ #\n",
    "print(\"\\nProtocolos em Test:\")\n",
    "print(df_test[\"Protocol\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd00f88-8708-49b6-8443-da99914616bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['Length'].describe())  # Min, max, mean, etc.\n",
    "print(\"============================\")\n",
    "print(df_test['Length'].describe())  # Min, max, mean, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666751a-63ab-4a1c-952f-836509553216",
   "metadata": {},
   "source": [
    "### Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7e8af-3da8-4b94-84ef-574bbfa72618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna({\"Info\": \"Unknown\"}, inplace=True)\n",
    "df_test.fillna({\"Info\": \"Unknown\"}, inplace=True)\n",
    "df_train.dropna(subset=['Source', 'Destination'], inplace=True)\n",
    "df_test.dropna(subset=['Source', 'Destination'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887ff82-55bb-40a5-b695-85346a0678c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter features categóricas \n",
    "df_train['Protocol'] = df_train['Protocol'].astype('category').cat.codes\n",
    "df_train['Source'] = df_train['Source'].astype('category').cat.codes\n",
    "df_train['Destination'] = df_train['Destination'].astype('category').cat.codes\n",
    "\n",
    "df_test['Protocol'] = df_test['Protocol'].astype('category').cat.codes\n",
    "df_test['Source'] = df_test['Source'].astype('category').cat.codes\n",
    "df_test['Destination'] = df_test['Destination'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405ef53-f66c-4ac6-bd5a-15e2292d6e3a",
   "metadata": {},
   "source": [
    "### Engenharia de *Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee900fca-2fe3-4617-9061-3b147ecef70a",
   "metadata": {},
   "source": [
    "ALTERAÇÕES FEITAS NAS FEATURES: \n",
    "\n",
    "    . Time_Diff é computada por IP Origem em vez de globalmente \n",
    "\n",
    "    . versão anterior de Packet_Rate usava \".count() / (x.max() - x.min())\" que pode\n",
    "    ser sensível a outliers. Nova versão usa IQR para excluir outliers extremos.\n",
    "\n",
    "    . Inter-Arrival Time passa a usar média móvel em vez de estática (Melhor eficiência)\n",
    "\n",
    "    . Usar \"np.where\" em Burst_Rate para salvaguardar em caso de Inter-Arrival Time = 0.\n",
    "\n",
    "    . Features com transformada logarítmica desaparecem e usa-se BoxCox - melhor para dados com distorção.\n",
    "\n",
    "    . Usar trasformada logarítimica em Inter-Arrival e Burst_Rate para melhorar para reduzir\n",
    "    o nível de distorção dos dados e manter a distribuição dos dados estável (teste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867c4d3-703b-44e3-89e9-efdd7c636599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================Time Diff Feature===================================================== #\n",
    "\n",
    "df_train['Time_Diff'] = df_train.groupby(\"Source\")['Time'].diff().fillna(df_train.groupby(\"Source\")['Time'].transform(\"mean\"))\n",
    "df_test['Time_Diff'] = df_test.groupby(\"Source\")['Time'].diff().fillna(df_test.groupby(\"Source\")['Time'].transform(\"mean\"))\n",
    "\n",
    "# =================================================Packet Rate Feature=================================================== #\n",
    "\n",
    "def packet_rate(x):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    x_clipped = np.clip(x, q1 - 1.5 * iqr, q3 + 1.5 * iqr)  # Cortar em vez de remover\n",
    "    return len(x_clipped) / (x_clipped.max() - x_clipped.min() + 1e-6)\n",
    "\n",
    "df_train['Packet_Rate'] = df_train.groupby(\"Source\")['Time'].transform(packet_rate)\n",
    "df_test['Packet_Rate'] = df_test.groupby(\"Source\")['Time'].transform(packet_rate)\n",
    "\n",
    "# =============================================Inter-Arrival Time Feature================================================ #\n",
    "# rolling = 10  - considera os 10 pacotes anteriores. Usar outros valores para ver o que é melhor \n",
    "\n",
    "df_train[\"Inter_Arrival_Time\"] = df_train.groupby(\"Source\")[\"Time_Diff\"].transform(lambda x: x.rolling(10, min_periods=1).mean())\n",
    "df_test[\"Inter_Arrival_Time\"] = df_test.groupby(\"Source\")[\"Time_Diff\"].transform(lambda x: x.rolling(10, min_periods=1).mean())\n",
    "\n",
    "df_train[\"Inter_Arrival_Time\"] = df_train[\"Inter_Arrival_Time\"].clip(lower=1e-6) # para resolver problemas com valores negativos\n",
    "df_test[\"Inter_Arrival_Time\"] = df_test[\"Inter_Arrival_Time\"].clip(lower=1e-6)\n",
    "\n",
    "df_train[\"Log_IATime\"] = np.log1p(df_train[\"Inter_Arrival_Time\"])\n",
    "df_test[\"Log_IATime\"] = np.log1p(df_test[\"Inter_Arrival_Time\"])\n",
    "\n",
    "# =================================================Burst Rate Feature==================================================== #\n",
    "\n",
    "df_train[\"Burst_Rate\"] = np.where(df_train[\"Inter_Arrival_Time\"] > 1e-6, 1 / df_train[\"Inter_Arrival_Time\"], 0)\n",
    "df_test[\"Burst_Rate\"] = np.where(df_test[\"Inter_Arrival_Time\"] > 1e-6, 1 / df_test[\"Inter_Arrival_Time\"], 0)\n",
    "\n",
    "df_train[\"Burst_Rate\"] = df_train[\"Burst_Rate\"].clip(lower=1e-6) # para resolver problemas com valores negativos\n",
    "df_test[\"Burst_Rate\"] = df_test[\"Burst_Rate\"].clip(lower=1e-6)\n",
    "\n",
    "df_train[\"Log_BRate\"] = np.log1p(df_train[\"Burst_Rate\"])\n",
    "df_test[\"Log_BRate\"] = np.log1p(df_test[\"Burst_Rate\"])\n",
    "\n",
    "# =================================================BoxCox Features======================================================= #\n",
    "# Antes: Log_Length\n",
    "df_train[\"BoxCox_Length\"], _ = boxcox(df_train[\"Length\"] + 1e-3) # leve desvio \n",
    "df_test[\"BoxCox_Length\"], _ = boxcox(df_test[\"Length\"] + 1e-3)\n",
    "# Antes: Log_Packet_Length\n",
    "df_train[\"BoxCox_PRate\"], _ = boxcox(df_train[\"Packet_Rate\"] + 1)\n",
    "df_test[\"BoxCox_PRate\"], _ = boxcox(df_test[\"Packet_Rate\"] + 1)\n",
    "\n",
    "# ======================================================================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3adc0e-c0c1-4c00-b5c9-b65fbdb78880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalonamento\n",
    "features = [\"Time_Diff\", \"Log_IATime\", \"Log_BRate\", \"BoxCox_Length\", \"BoxCox_PRate\"]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train[features])\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train[features]), columns=features)\n",
    "df_test_scaled = pd.DataFrame(scaler.transform(df_test[features]), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1972a-6223-4571-aa56-3ea3ad9a745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipular valores duplicados (problemas anteriores com duplicação de valores)\n",
    "\n",
    "df_train_scaled = df_train_scaled.drop_duplicates()\n",
    "df_test_scaled = df_test_scaled.drop_duplicates()\n",
    "\n",
    "# adicionar ruído para tratar valores muito idênticos \n",
    "df_train_scaled += np.random.normal(0, 1e-6, df_train_scaled.shape)\n",
    "df_test_scaled += np.random.normal(0, 1e-6, df_test_scaled.shape)\n",
    "\n",
    "# reduzir precisão (motivos de recursos computacionais)\n",
    "df_train_scaled = df_train_scaled.astype(\"float32\")\n",
    "df_test_scaled = df_test_scaled.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8007c8-31c8-4490-850b-29101e3c6ef8",
   "metadata": {},
   "source": [
    "### Treinar primeiro modelo (*Isolation Forest*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29e25-933f-4a34-b120-736b511a0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest (IForest)\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=550,\n",
    "    contamination=\"auto\",\n",
    "    random_state=45,\n",
    "    verbose=0) \n",
    "\n",
    "iso_forest.fit(df_train_scaled)\n",
    "df_train_scaled[\"Anomaly_IForest\"] = iso_forest.predict(df_train_scaled)\n",
    "df_test_scaled[\"Anomaly_IForest\"] = iso_forest.predict(df_test_scaled)\n",
    "\n",
    "print(f\"Anomalias em Training (IForest): {(df_train_scaled['Anomaly_IForest'] == -1).sum() / len(df_train_scaled) * 100:.2f}%\")\n",
    "print(f\"Anomalias em Testing (IForest): {(df_test_scaled['Anomaly_IForest'] == -1).sum() / len(df_test_scaled) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8ae5c-74a9-4eed-a643-989ac7d69035",
   "metadata": {},
   "source": [
    "### Treinar segundo modelo (OCSVM, AutoEncoder, -> LOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00dbef-2e79-4021-baed-286a8e23200c",
   "metadata": {},
   "source": [
    "CONSIDERAÇÕES:\n",
    "\n",
    "n_neighbors:\n",
    "quanto maior o valor da vizinhança, maior o consumo de memória. \n",
    "Este consumo pode crescer rapidamente, ainda mais com o uso de muitos dados,\n",
    "então usamos apenas uma amostra dos dados, para treinar o algoritmo e não \n",
    "usar tanto poder computacional.\n",
    "    \n",
    "    -> usar valores altos se anomalias forem dispersas globalmente, e valores baixos se anomalias forem densas localmente\n",
    "    -> experimentar: 30, 50 (default), 100, *150*\n",
    "\n",
    "metrics: \n",
    "\n",
    "    euclidean- default, \n",
    "    \n",
    "    chebyshev - network logs, \n",
    "    \n",
    "    manhattan - packet-based data\n",
    "\n",
    "* Aumenta 10% por cada vez que aumenta 'contamination':\n",
    "  \n",
    "    -> 0.1 - 10%, 0.2 - 20%, 0.3 - 30% ...\n",
    "  \n",
    "  Usei \"auto\" para não forçar anomalias.\n",
    "\n",
    "  * melhor métrica: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635be77-9964-471b-84de-981eee0b5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Outlier Factor (LOF)\n",
    "batch_size = 200_000\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=130,\n",
    "    #algorithm=\"kd_tree\", # a usar: \"ball_tree\" ou \"kd_tree\". Algoritmos de pesquisa k-NN\n",
    "    #leaf_size=100, # para eficiência de pesquisa do algoritmo\n",
    "    contamination=\"auto\", \n",
    "    metric=\"manhattan\",\n",
    "    #novelty=False,\n",
    "    n_jobs=-1)\n",
    "\n",
    "def lof_predict(data, batch_size, lof_model):\n",
    "    \n",
    "    y_pred = np.zeros(len(data))\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data.iloc[i:i + batch_size]  \n",
    "        y_pred[i:i + batch_size] = lof_model.fit_predict(batch) \n",
    "\n",
    "    return y_pred\n",
    "\n",
    "df_train_scaled['Anomaly_LOF'] = lof_predict(df_train_scaled, batch_size, lof)\n",
    "df_test_scaled['Anomaly_LOF'] = lof_predict(df_test_scaled, batch_size, lof)\n",
    "\n",
    "print(f\"Anomalias em Training (LOF): {(df_train_scaled['Anomaly_LOF'] == -1).sum() / len(df_train_scaled) * 100:.2f}%\")\n",
    "print(f\"Anomalias em Testing (LOF): {(df_test_scaled['Anomaly_LOF'] == -1).sum() / len(df_test_scaled) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b85d13-8ec9-4d76-8576-aeebf879d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nível de concordância entre os modelos\n",
    "\n",
    "overlap = (df_train_scaled[\"Anomaly_LOF\"] == df_train_scaled[\"Anomaly_IForest\"]).mean()\n",
    "print(f\"Concordância entre LOF e Isolation Forest: {overlap:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805b3c6-e8ea-474d-834b-863c3ea8ddbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualização gráfica dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e8877-7022-4999-bb2d-3af08d2e7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de distribuição de features,ç\n",
    "features = df_train_scaled.columns[:-1]  # Exclui rótulos de anomalias\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.histplot(df_train_scaled[feature], bins=50, kde=True)\n",
    "    plt.title(f\"Histograma: {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca939012-660d-42b6-8073-83ac22102883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de correlação\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_train_scaled.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Mapa de Correlação\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe3eaf-5b41-4ccb-b188-f5b379d72155",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"Log_IATime\", \"Log_BRate\", \"BoxCox_Length\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(selected_features), 2, figsize=(14, 5 * len(selected_features)))\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    # Isolation Forest KDE\n",
    "    sns.kdeplot(df_train_scaled[df_train_scaled[\"Anomaly_IForest\"] == 1][feature], \n",
    "                label=\"Normal\", color=\"blue\", ax=axes[i, 0], fill=True)\n",
    "    sns.kdeplot(df_train_scaled[df_train_scaled[\"Anomaly_IForest\"] == -1][feature], \n",
    "                label=\"Anomaly\", color=\"red\", ax=axes[i, 0], fill=True)\n",
    "    axes[i, 0].set_title(f\"KDE Plot - IForest ({feature})\")\n",
    "    axes[i, 0].legend()\n",
    "    \n",
    "    # Local Outlier Factor KDE\n",
    "    sns.kdeplot(df_train_scaled[df_train_scaled[\"Anomaly_LOF\"] == 1][feature], \n",
    "                label=\"Normal\", color=\"blue\", ax=axes[i, 1], fill=True)\n",
    "    sns.kdeplot(df_train_scaled[df_train_scaled[\"Anomaly_LOF\"] == -1][feature], \n",
    "                label=\"Anomaly\", color=\"red\", ax=axes[i, 1], fill=True)\n",
    "    axes[i, 1].set_title(f\"KDE Plot - LOF ({feature})\")\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647525a-285b-4bcf-b05f-ddc6e45b2f13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Métrica de avaliação de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3112e8f-8831-4212-bed6-a81eadaf6c0b",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    " * **Como funciona:**\n",
    "   \n",
    "    -> Para cada ponto no dataset\n",
    "   \n",
    "   calcula a distância ao seu cluster (coesão).\n",
    "   \n",
    "   calcula a distância aos outros clusters (distinção)\n",
    "\n",
    "    -> Funciona no intervalo de -1 a 1\n",
    "\n",
    "   -1 é péssima distinção.\n",
    "   \n",
    "   0 é porque existe sobreposição de valores.\n",
    "   \n",
    "   1 é perfeita distinção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfa27d-4325-493f-bdac-060c3497840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 50_000\n",
    "num_batches = len(df_train_scaled) // sample  # Número de partições\n",
    "\n",
    "# Listas\n",
    "sil_scores_if_train = []\n",
    "sil_scores_if_test = []\n",
    "sil_scores_lof_train = []\n",
    "sil_scores_lof_test = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    print(f\"A processar partição {i+1}/{num_batches}...\")\n",
    "\n",
    "    train_batch = df_train_scaled.iloc[i * sample : (i + 1) * sample]\n",
    "    test_batch = df_test_scaled.iloc[i * sample : (i + 1) * sample]\n",
    "\n",
    "    sil_scores_if_train.append(silhouette_score(train_batch, train_batch[\"Anomaly_IForest\"], metric=\"manhattan\"))\n",
    "    sil_scores_if_test.append(silhouette_score(test_batch, test_batch[\"Anomaly_IForest\"], metric=\"manhattan\"))\n",
    "\n",
    "    sil_scores_lof_train.append(silhouette_score(train_batch, train_batch[\"Anomaly_LOF\"], metric=\"manhattan\"))\n",
    "    sil_scores_lof_test.append(silhouette_score(test_batch, test_batch[\"Anomaly_LOF\"], metric=\"manhattan\"))\n",
    "\n",
    "# Médias\n",
    "avg_sil_if_train = np.mean(sil_scores_if_train)\n",
    "avg_sil_if_test = np.mean(sil_scores_if_test)\n",
    "avg_sil_lof_train = np.mean(sil_scores_lof_train)\n",
    "avg_sil_lof_test = np.mean(sil_scores_lof_test)\n",
    "\n",
    "print(f\"\\nAvaliação final:\")\n",
    "print(f\"I-Forest -> Train: {avg_sil_if_train:.4f}%\")\n",
    "print(f\"I-Forest -> Test: {avg_sil_if_test:.4f}%\")\n",
    "print(f\"LOF -> Train: {avg_sil_lof_train:.4f}%\")\n",
    "print(f\"LOF -> Test: {avg_sil_lof_test:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
